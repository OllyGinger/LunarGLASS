
Top IR:
; ModuleID = 'Glslang'

@uv4 = external addrspace(2) constant <4 x float>
@ui = external addrspace(2) constant i32
@uf = external addrspace(2) constant float
@ub41 = external addrspace(2) constant <4 x i1>
@ub42 = external addrspace(2) constant <4 x i1>
@gl_FragColor = global <4 x float> zeroinitializer
@uiv4 = external addrspace(2) constant <4 x i32>
@ub = external addrspace(2) constant i1

define fastcc void @main() {
entry:
  %constructed75 = alloca <4 x float>
  %constructed = alloca <4 x float>
  %ternary = alloca <4 x float>
  %f = alloca float
  %b = alloca i1
  %i = alloca i32
  %v = alloca <4 x float>
  %0 = load <4 x float> addrspace(2)* @uv4, !gla.uniform !1
  %v1 = call <4 x float> @llvm.gla.fRadians.v4f32.v4f32(<4 x float> %0)
  store <4 x float> %v1, <4 x float>* %v
  %1 = load <4 x float>* %v
  %2 = call <4 x float> @llvm.gla.fDegrees.v4f32.v4f32(<4 x float> %1)
  %3 = load <4 x float>* %v
  %v2 = fadd <4 x float> %3, %2
  store <4 x float> %v2, <4 x float>* %v
  %4 = load i32 addrspace(2)* @ui, !gla.uniform !3
  %5 = load i32 addrspace(2)* @ui, !gla.uniform !3
  %i3 = mul i32 %4, %5
  store i32 %i3, i32* %i
  %6 = load <4 x float>* %v
  %7 = call <4 x float> @llvm.gla.fSin.v4f32.v4f32(<4 x float> %6)
  %8 = load <4 x float>* %v
  %v4 = fadd <4 x float> %8, %7
  store <4 x float> %v4, <4 x float>* %v
  %9 = load <4 x float>* %v
  %10 = call <4 x float> @llvm.gla.fCos.v4f32.v4f32(<4 x float> %9)
  %11 = load <4 x float>* %v
  %v5 = fadd <4 x float> %11, %10
  store <4 x float> %v5, <4 x float>* %v
  %12 = load <4 x float>* %v
  %13 = call <4 x float> @llvm.gla.fTan.v4f32.v4f32(<4 x float> %12)
  %14 = load <4 x float>* %v
  %v6 = fadd <4 x float> %14, %13
  store <4 x float> %v6, <4 x float>* %v
  %15 = load <4 x float>* %v
  %16 = call <4 x float> @llvm.gla.fAsin.v4f32.v4f32(<4 x float> %15)
  %17 = load <4 x float>* %v
  %v7 = fadd <4 x float> %17, %16
  store <4 x float> %v7, <4 x float>* %v
  %18 = load <4 x float>* %v
  %19 = call <4 x float> @llvm.gla.fAcos.v4f32.v4f32(<4 x float> %18)
  %20 = load <4 x float>* %v
  %v8 = fadd <4 x float> %20, %19
  store <4 x float> %v8, <4 x float>* %v
  %21 = load <4 x float>* %v
  %22 = call <4 x float> @llvm.gla.fAtan.v4f32.v4f32(<4 x float> %21)
  %23 = load <4 x float>* %v
  %v9 = fadd <4 x float> %23, %22
  store <4 x float> %v9, <4 x float>* %v
  %24 = load <4 x float>* %v
  %25 = call <4 x float> @llvm.gla.fSinh.v4f32.v4f32(<4 x float> %24)
  %26 = load <4 x float>* %v
  %v10 = fadd <4 x float> %26, %25
  store <4 x float> %v10, <4 x float>* %v
  %27 = load <4 x float>* %v
  %28 = call <4 x float> @llvm.gla.fCosh.v4f32.v4f32(<4 x float> %27)
  %29 = load <4 x float>* %v
  %v11 = fadd <4 x float> %29, %28
  store <4 x float> %v11, <4 x float>* %v
  %30 = load <4 x float>* %v
  %31 = call <4 x float> @llvm.gla.fTanh.v4f32.v4f32(<4 x float> %30)
  %32 = load <4 x float>* %v
  %v12 = fadd <4 x float> %32, %31
  store <4 x float> %v12, <4 x float>* %v
  %33 = load <4 x float>* %v
  %34 = call <4 x float> @llvm.gla.fAsinh.v4f32.v4f32(<4 x float> %33)
  %35 = load <4 x float>* %v
  %v13 = fadd <4 x float> %35, %34
  store <4 x float> %v13, <4 x float>* %v
  %36 = load <4 x float>* %v
  %37 = call <4 x float> @llvm.gla.fAcosh.v4f32.v4f32(<4 x float> %36)
  %38 = load <4 x float>* %v
  %v14 = fadd <4 x float> %38, %37
  store <4 x float> %v14, <4 x float>* %v
  %39 = load <4 x float>* %v
  %40 = call <4 x float> @llvm.gla.fAtanh.v4f32.v4f32(<4 x float> %39)
  %41 = load <4 x float>* %v
  %v15 = fadd <4 x float> %41, %40
  store <4 x float> %v15, <4 x float>* %v
  %42 = load <4 x float>* %v
  %43 = load <4 x float>* %v
  %44 = call <4 x float> @llvm.gla.fPow.v4f32.v4f32.v4f32(<4 x float> %42, <4 x float> %43)
  %45 = load <4 x float>* %v
  %v16 = fadd <4 x float> %45, %44
  store <4 x float> %v16, <4 x float>* %v
  %46 = load <4 x float>* %v
  %47 = call <4 x float> @llvm.gla.fExp.v4f32.v4f32(<4 x float> %46)
  %48 = load <4 x float>* %v
  %v17 = fadd <4 x float> %48, %47
  store <4 x float> %v17, <4 x float>* %v
  %49 = load <4 x float>* %v
  %50 = call <4 x float> @llvm.gla.fLog.v4f32.v4f32(<4 x float> %49)
  %51 = load <4 x float>* %v
  %v18 = fadd <4 x float> %51, %50
  store <4 x float> %v18, <4 x float>* %v
  %52 = load <4 x float>* %v
  %53 = call <4 x float> @llvm.gla.fExp2.v4f32.v4f32(<4 x float> %52)
  %54 = load <4 x float>* %v
  %v19 = fadd <4 x float> %54, %53
  store <4 x float> %v19, <4 x float>* %v
  %55 = load <4 x float>* %v
  %56 = call <4 x float> @llvm.gla.fLog2.v4f32.v4f32(<4 x float> %55)
  %57 = load <4 x float>* %v
  %v20 = fadd <4 x float> %57, %56
  store <4 x float> %v20, <4 x float>* %v
  %58 = load <4 x float>* %v
  %59 = call <4 x float> @llvm.gla.fSqrt.v4f32.v4f32(<4 x float> %58)
  %60 = load <4 x float>* %v
  %v21 = fadd <4 x float> %60, %59
  store <4 x float> %v21, <4 x float>* %v
  %61 = load <4 x float>* %v
  %62 = call <4 x float> @llvm.gla.fInverseSqrt.v4f32.v4f32(<4 x float> %61)
  %63 = load <4 x float>* %v
  %v22 = fadd <4 x float> %63, %62
  store <4 x float> %v22, <4 x float>* %v
  %64 = load <4 x float>* %v
  %65 = call <4 x float> @llvm.gla.fAbs.v4f32.v4f32(<4 x float> %64)
  %66 = load <4 x float>* %v
  %v23 = fadd <4 x float> %66, %65
  store <4 x float> %v23, <4 x float>* %v
  %67 = load <4 x float>* %v
  %68 = call <4 x float> @llvm.gla.fSign.v4f32.v4f32(<4 x float> %67)
  %69 = load <4 x float>* %v
  %v24 = fadd <4 x float> %69, %68
  store <4 x float> %v24, <4 x float>* %v
  %70 = load <4 x float>* %v
  %71 = call <4 x float> @llvm.gla.fFloor.v4f32.v4f32(<4 x float> %70)
  %72 = load <4 x float>* %v
  %v25 = fadd <4 x float> %72, %71
  store <4 x float> %v25, <4 x float>* %v
  %73 = load <4 x float>* %v
  %74 = call <4 x float> @llvm.gla.fCeiling.v4f32.v4f32(<4 x float> %73)
  %75 = load <4 x float>* %v
  %v26 = fadd <4 x float> %75, %74
  store <4 x float> %v26, <4 x float>* %v
  %76 = load <4 x float>* %v
  %77 = call <4 x float> @llvm.gla.fFraction.v4f32.v4f32(<4 x float> %76)
  %78 = load <4 x float>* %v
  %v27 = fadd <4 x float> %78, %77
  store <4 x float> %v27, <4 x float>* %v
  %79 = load <4 x float>* %v
  %80 = load <4 x float>* %v
  %81 = frem <4 x float> %79, %80
  %82 = load <4 x float>* %v
  %v28 = fadd <4 x float> %82, %81
  store <4 x float> %v28, <4 x float>* %v
  %83 = load <4 x float>* %v
  %84 = load <4 x float>* %v
  %85 = extractelement <4 x float> %84, i32 0
  %86 = insertelement <4 x float> undef, float %85, i32 0
  %87 = insertelement <4 x float> %86, float %85, i32 1
  %88 = insertelement <4 x float> %87, float %85, i32 2
  %89 = insertelement <4 x float> %88, float %85, i32 3
  %90 = frem <4 x float> %83, %89
  %91 = load <4 x float>* %v
  %v29 = fadd <4 x float> %91, %90
  store <4 x float> %v29, <4 x float>* %v
  %92 = load <4 x float>* %v
  %93 = load <4 x float> addrspace(2)* @uv4, !gla.uniform !1
  %94 = call <4 x float> @llvm.gla.fMin.v4f32.v4f32.v4f32(<4 x float> %92, <4 x float> %93)
  %95 = load <4 x float>* %v
  %v30 = fadd <4 x float> %95, %94
  store <4 x float> %v30, <4 x float>* %v
  %96 = load <4 x float>* %v
  %97 = load <4 x float> addrspace(2)* @uv4, !gla.uniform !1
  %98 = call <4 x float> @llvm.gla.fMax.v4f32.v4f32.v4f32(<4 x float> %96, <4 x float> %97)
  %99 = load <4 x float>* %v
  %v31 = fadd <4 x float> %99, %98
  store <4 x float> %v31, <4 x float>* %v
  %100 = load <4 x float>* %v
  %101 = load <4 x float> addrspace(2)* @uv4, !gla.uniform !1
  %102 = load <4 x float> addrspace(2)* @uv4, !gla.uniform !1
  %103 = call <4 x float> @llvm.gla.fClamp.v4f32.v4f32.v4f32.v4f32(<4 x float> %100, <4 x float> %101, <4 x float> %102)
  %104 = load <4 x float>* %v
  %v32 = fadd <4 x float> %104, %103
  store <4 x float> %v32, <4 x float>* %v
  %105 = load <4 x float>* %v
  %106 = load <4 x float>* %v
  %107 = load <4 x float>* %v
  %108 = call <4 x float> @llvm.gla.fMix.v4f32.v4f32.v4f32.v4f32(<4 x float> %105, <4 x float> %106, <4 x float> %107)
  %109 = load <4 x float>* %v
  %v33 = fadd <4 x float> %109, %108
  store <4 x float> %v33, <4 x float>* %v
  %110 = load <4 x float>* %v
  %111 = load <4 x float>* %v
  %112 = call <4 x float> @llvm.gla.fStep.v4f32.v4f32.v4f32(<4 x float> %110, <4 x float> %111)
  %113 = load <4 x float>* %v
  %v34 = fadd <4 x float> %113, %112
  store <4 x float> %v34, <4 x float>* %v
  %114 = load <4 x float>* %v
  %115 = load <4 x float>* %v
  %116 = load <4 x float>* %v
  %117 = call <4 x float> @llvm.gla.fSmoothStep.v4f32.v4f32.v4f32.v4f32(<4 x float> %114, <4 x float> %115, <4 x float> %116)
  %118 = load <4 x float>* %v
  %v35 = fadd <4 x float> %118, %117
  store <4 x float> %v35, <4 x float>* %v
  %119 = load float addrspace(2)* @uf, !gla.uniform !4
  %120 = load <4 x float>* %v
  %121 = call <4 x float> @llvm.gla.fStep.v4f32.f32.v4f32(float %119, <4 x float> %120)
  %122 = load <4 x float>* %v
  %v36 = fadd <4 x float> %122, %121
  store <4 x float> %v36, <4 x float>* %v
  %123 = load float addrspace(2)* @uf, !gla.uniform !4
  %124 = load float addrspace(2)* @uf, !gla.uniform !4
  %125 = load <4 x float>* %v
  %126 = call <4 x float> @llvm.gla.fSmoothStep.v4f32.f32.f32.v4f32(float %123, float %124, <4 x float> %125)
  %127 = load <4 x float>* %v
  %v37 = fadd <4 x float> %127, %126
  store <4 x float> %v37, <4 x float>* %v
  %128 = load <4 x float>* %v
  %129 = call <4 x float> @llvm.gla.fNormalize.v4f32.v4f32(<4 x float> %128)
  %130 = load <4 x float>* %v
  %v38 = fadd <4 x float> %130, %129
  store <4 x float> %v38, <4 x float>* %v
  %131 = load <4 x float>* %v
  %132 = load <4 x float>* %v
  %133 = load <4 x float>* %v
  %134 = call <4 x float> @llvm.gla.fFaceForward.v4f32.v4f32.v4f32.v4f32(<4 x float> %131, <4 x float> %132, <4 x float> %133)
  %135 = load <4 x float>* %v
  %v39 = fadd <4 x float> %135, %134
  store <4 x float> %v39, <4 x float>* %v
  %136 = load <4 x float>* %v
  %137 = load <4 x float>* %v
  %138 = call <4 x float> @llvm.gla.fReflect.v4f32.v4f32.v4f32(<4 x float> %136, <4 x float> %137)
  %139 = load <4 x float>* %v
  %v40 = fadd <4 x float> %139, %138
  store <4 x float> %v40, <4 x float>* %v
  %140 = load <4 x float>* %v
  %141 = load <4 x float>* %v
  %142 = load float addrspace(2)* @uf, !gla.uniform !4
  %143 = call <4 x float> @llvm.gla.fRefract.v4f32.v4f32.v4f32.f32(<4 x float> %140, <4 x float> %141, float %142)
  %144 = load <4 x float>* %v
  %v41 = fadd <4 x float> %144, %143
  store <4 x float> %v41, <4 x float>* %v
  %145 = load <4 x float>* %v
  %146 = call <4 x float> @llvm.gla.fDFdx.v4f32.v4f32(<4 x float> %145)
  %147 = load <4 x float>* %v
  %v42 = fadd <4 x float> %147, %146
  store <4 x float> %v42, <4 x float>* %v
  %148 = load <4 x float>* %v
  %149 = call <4 x float> @llvm.gla.fDFdy.v4f32.v4f32(<4 x float> %148)
  %150 = load <4 x float>* %v
  %v43 = fadd <4 x float> %150, %149
  store <4 x float> %v43, <4 x float>* %v
  %151 = load <4 x float>* %v
  %152 = call <4 x float> @llvm.gla.fFilterWidth.v4f32.v4f32(<4 x float> %151)
  %153 = load <4 x float>* %v
  %v44 = fadd <4 x float> %153, %152
  store <4 x float> %v44, <4 x float>* %v
  %154 = load <4 x float>* %v
  %155 = load <4 x float> addrspace(2)* @uv4, !gla.uniform !1
  %156 = fcmp olt <4 x float> %154, %155
  %b45 = call i1 @llvm.gla.any.v4i1(<4 x i1> %156)
  store i1 %b45, i1* %b
  %157 = load i1* %b
  %158 = load <4 x float>* %v
  %159 = load <4 x float> addrspace(2)* @uv4, !gla.uniform !1
  %160 = fcmp ole <4 x float> %158, %159
  %161 = call i1 @llvm.gla.any.v4i1(<4 x i1> %160)
  %b46 = and i1 %157, %161
  store i1 %b46, i1* %b
  %162 = load i1* %b
  %163 = load <4 x float>* %v
  %164 = load <4 x float> addrspace(2)* @uv4, !gla.uniform !1
  %165 = fcmp ogt <4 x float> %163, %164
  %166 = call i1 @llvm.gla.any.v4i1(<4 x i1> %165)
  %b47 = and i1 %162, %166
  store i1 %b47, i1* %b
  %167 = load i1* %b
  %168 = load <4 x float>* %v
  %169 = load <4 x float> addrspace(2)* @uv4, !gla.uniform !1
  %170 = fcmp oge <4 x float> %168, %169
  %171 = call i1 @llvm.gla.any.v4i1(<4 x i1> %170)
  %b48 = and i1 %167, %171
  store i1 %b48, i1* %b
  %172 = load i1* %b
  %173 = load <4 x i1> addrspace(2)* @ub41, !gla.uniform !5
  %174 = load <4 x i1> addrspace(2)* @ub42, !gla.uniform !6
  %175 = icmp eq <4 x i1> %173, %174
  %176 = call i1 @llvm.gla.any.v4i1(<4 x i1> %175)
  %b49 = and i1 %172, %176
  store i1 %b49, i1* %b
  %177 = load i1* %b
  %178 = load <4 x i1> addrspace(2)* @ub41, !gla.uniform !5
  %179 = load <4 x i1> addrspace(2)* @ub42, !gla.uniform !6
  %180 = icmp ne <4 x i1> %178, %179
  %181 = call i1 @llvm.gla.any.v4i1(<4 x i1> %180)
  %b50 = and i1 %177, %181
  store i1 %b50, i1* %b
  %182 = load i1* %b
  %183 = load <4 x i1> addrspace(2)* @ub41, !gla.uniform !5
  %184 = call i1 @llvm.gla.any.v4i1(<4 x i1> %183)
  %b51 = and i1 %182, %184
  store i1 %b51, i1* %b
  %185 = load i1* %b
  %186 = load <4 x i1> addrspace(2)* @ub41, !gla.uniform !5
  %187 = call i1 @llvm.gla.all.v4i1(<4 x i1> %186)
  %b52 = and i1 %185, %187
  store i1 %b52, i1* %b
  %188 = load i1* %b
  %189 = load <4 x i1> addrspace(2)* @ub41, !gla.uniform !5
  %190 = xor <4 x i1> %189, <i1 true, i1 true, i1 true, i1 true>
  %191 = call i1 @llvm.gla.any.v4i1(<4 x i1> %190)
  %b53 = and i1 %188, %191
  store i1 %b53, i1* %b
  %192 = load i32* %i
  %193 = load i32 addrspace(2)* @ui, !gla.uniform !3
  %194 = add i32 %192, %193
  %195 = load i32* %i
  %196 = mul i32 %194, %195
  %197 = load i32 addrspace(2)* @ui, !gla.uniform !3
  %198 = sub i32 %196, %197
  %199 = load i32* %i
  %i54 = sdiv i32 %198, %199
  store i32 %i54, i32* %i
  %200 = load i32* %i
  %201 = load i32 addrspace(2)* @ui, !gla.uniform !3
  %i55 = srem i32 %200, %201
  store i32 %i55, i32* %i
  %202 = load i32* %i
  %203 = load i32 addrspace(2)* @ui, !gla.uniform !3
  %204 = icmp eq i32 %202, %203
  %205 = load i32* %i
  %206 = load i32 addrspace(2)* @ui, !gla.uniform !3
  %207 = icmp ne i32 %205, %206
  %208 = load i32* %i
  %209 = load i32 addrspace(2)* @ui, !gla.uniform !3
  %210 = icmp eq i32 %208, %209
  %211 = and i1 %207, %210
  %212 = load i32* %i
  %213 = icmp ne i32 %212, 2
  %214 = xor i1 %211, %213
  %215 = or i1 %204, %214
  br i1 %215, label %then, label %ifmerge

then:                                             ; preds = %entry
  %216 = load i32* %i
  %i56 = add i32 %216, 1
  store i32 %i56, i32* %i
  br label %ifmerge

ifmerge:                                          ; preds = %entry, %then
  %217 = load float addrspace(2)* @uf, !gla.uniform !4
  %218 = load float addrspace(2)* @uf, !gla.uniform !4
  %219 = fadd float %217, %218
  %220 = load float addrspace(2)* @uf, !gla.uniform !4
  %221 = fmul float %219, %220
  %222 = load float addrspace(2)* @uf, !gla.uniform !4
  %223 = fsub float %221, %222
  %224 = load float addrspace(2)* @uf, !gla.uniform !4
  %f57 = fdiv float %223, %224
  store float %f57, float* %f
  %225 = load <4 x float>* %v
  %226 = call float @llvm.gla.fLength.f32.v4f32(<4 x float> %225)
  %227 = load float* %f
  %f58 = fadd float %227, %226
  store float %f58, float* %f
  %228 = load <4 x float>* %v
  %229 = load <4 x float>* %v
  %230 = call float @llvm.gla.fDistance.f32.v4f32.v4f32(<4 x float> %228, <4 x float> %229)
  %231 = load float* %f
  %f59 = fadd float %231, %230
  store float %f59, float* %f
  %232 = load <4 x float>* %v
  %233 = load <4 x float>* %v
  %234 = call float @llvm.gla.fDot4.f32.v4f32.v4f32(<4 x float> %232, <4 x float> %233)
  %235 = load float* %f
  %f60 = fadd float %235, %234
  store float %f60, float* %f
  %236 = load float* %f
  %237 = load float addrspace(2)* @uf, !gla.uniform !4
  %238 = fmul float %236, %237
  %239 = load float* %f
  %f61 = fadd float %239, %238
  store float %f61, float* %f
  %240 = load <4 x float>* %v
  %241 = extractelement <4 x float> %240, i32 0
  %242 = insertelement <3 x float> undef, float %241, i32 0
  %243 = extractelement <4 x float> %240, i32 1
  %244 = insertelement <3 x float> %242, float %243, i32 1
  %245 = extractelement <4 x float> %240, i32 2
  %246 = insertelement <3 x float> %244, float %245, i32 2
  %247 = load <4 x float>* %v
  %248 = extractelement <4 x float> %247, i32 0
  %249 = insertelement <3 x float> undef, float %248, i32 0
  %250 = extractelement <4 x float> %247, i32 1
  %251 = insertelement <3 x float> %249, float %250, i32 1
  %252 = extractelement <4 x float> %247, i32 2
  %253 = insertelement <3 x float> %251, float %252, i32 2
  %254 = call <3 x float> @llvm.gla.fCross.v3f32.v3f32.v3f32(<3 x float> %246, <3 x float> %253)
  %255 = extractelement <3 x float> %254, i32 0
  %256 = load float* %f
  %f62 = fadd float %256, %255
  store float %f62, float* %f
  %257 = load float* %f
  %258 = load float addrspace(2)* @uf, !gla.uniform !4
  %259 = fcmp oeq float %257, %258
  %260 = load float* %f
  %261 = load float addrspace(2)* @uf, !gla.uniform !4
  %262 = fcmp one float %260, %261
  %263 = load float* %f
  %264 = fcmp one float %263, 2.000000e+00
  %265 = and i1 %262, %264
  %266 = or i1 %259, %265
  br i1 %266, label %then63, label %ifmerge65

then63:                                           ; preds = %ifmerge
  %267 = load float* %f
  %f64 = fadd float %267, 1.000000e+00
  store float %f64, float* %f
  br label %ifmerge65

ifmerge65:                                        ; preds = %ifmerge, %then63
  %268 = load i32 addrspace(2)* @ui, !gla.uniform !3
  %269 = load i32* %i
  %i66 = and i32 %269, %268
  store i32 %i66, i32* %i
  %270 = load i32* %i
  %i67 = or i32 %270, 66
  store i32 %i67, i32* %i
  %271 = load i32 addrspace(2)* @ui, !gla.uniform !3
  %272 = load i32* %i
  %i68 = xor i32 %272, %271
  store i32 %i68, i32* %i
  %273 = load i32* %i
  %i69 = srem i32 %273, 17
  store i32 %i69, i32* %i
  %274 = load i32* %i
  %i70 = ashr i32 %274, 2
  store i32 %i70, i32* %i
  %275 = load i32 addrspace(2)* @ui, !gla.uniform !3
  %276 = load i32* %i
  %i71 = shl i32 %276, %275
  store i32 %i71, i32* %i
  %277 = load i32* %i
  %i72 = xor i32 %277, -1
  store i32 %i72, i32* %i
  %278 = load i1* %b
  %b73 = xor i1 %278, true
  store i1 %b73, i1* %b
  %279 = load i1* %b
  br i1 %279, label %then74, label %else

then74:                                           ; preds = %ifmerge65
  %280 = load i32* %i
  %281 = sitofp i32 %280 to float
  %282 = load <4 x float>* %constructed
  %283 = insertelement <4 x float> undef, float %281, i32 0
  %284 = insertelement <4 x float> %283, float %281, i32 1
  %285 = insertelement <4 x float> %284, float %281, i32 2
  %286 = insertelement <4 x float> %285, float %281, i32 3
  %287 = load float* %f
  %288 = load <4 x float>* %constructed75
  %289 = insertelement <4 x float> undef, float %287, i32 0
  %290 = insertelement <4 x float> %289, float %287, i32 1
  %291 = insertelement <4 x float> %290, float %287, i32 2
  %292 = insertelement <4 x float> %291, float %287, i32 3
  %293 = fadd <4 x float> %286, %292
  %294 = load <4 x float>* %v
  %ternary76 = fadd <4 x float> %293, %294
  store <4 x float> %ternary76, <4 x float>* %ternary
  br label %ifmerge78

else:                                             ; preds = %ifmerge65
  %ternary77 = load <4 x float>* %v
  store <4 x float> %ternary77, <4 x float>* %ternary
  br label %ifmerge78

ifmerge78:                                        ; preds = %else, %then74
  %gl_FragColor = load <4 x float>* %ternary
  store <4 x float> %gl_FragColor, <4 x float>* @gl_FragColor
  br label %stage-epilogue

stage-epilogue:                                   ; preds = %ifmerge78
  br label %stage-exit

stage-exit:                                       ; preds = %stage-epilogue
  ret void
}

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fRadians.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fDegrees.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSin.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fCos.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fTan.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAsin.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAcos.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAtan.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSinh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fCosh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fTanh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAsinh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAcosh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAtanh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fPow.v4f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fExp.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fLog.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fExp2.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fLog2.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSqrt.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fInverseSqrt.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAbs.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSign.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fFloor.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fCeiling.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fFraction.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fMin.v4f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fMax.v4f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fClamp.v4f32.v4f32.v4f32.v4f32(<4 x float>, <4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fMix.v4f32.v4f32.v4f32.v4f32(<4 x float>, <4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fStep.v4f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSmoothStep.v4f32.v4f32.v4f32.v4f32(<4 x float>, <4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fStep.v4f32.f32.v4f32(float, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSmoothStep.v4f32.f32.f32.v4f32(float, float, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fNormalize.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fFaceForward.v4f32.v4f32.v4f32.v4f32(<4 x float>, <4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fReflect.v4f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fRefract.v4f32.v4f32.v4f32.f32(<4 x float>, <4 x float>, float) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fDFdx.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fDFdy.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fFilterWidth.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare i1 @llvm.gla.any.v4i1(<4 x i1>) #0

; Function Attrs: nounwind readnone
declare i1 @llvm.gla.all.v4i1(<4 x i1>) #0

; Function Attrs: nounwind readnone
declare float @llvm.gla.fLength.f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare float @llvm.gla.fDistance.f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare float @llvm.gla.fDot4.f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <3 x float> @llvm.gla.fCross.v3f32.v3f32.v3f32(<3 x float>, <3 x float>) #0

attributes #0 = { nounwind readnone }

!gla.entrypoint = !{!0}
!gla.uniforms = !{!1, !3, !4, !5, !6, !7, !8}
!gla.outputs = !{!9}
!gla.noStaticUse = !{!7, !8}

!0 = metadata !{metadata !"main", i32 15}
!1 = metadata !{metadata !"uv4", i32 12, <4 x float>* @uv4_typeProxy, metadata !2}
!2 = metadata !{i32 0, i32 0, i32 1024, null}
!3 = metadata !{metadata !"ui", i32 12, i32* @ui_typeProxy, metadata !2}
!4 = metadata !{metadata !"uf", i32 12, float* @uf_typeProxy, metadata !2}
!5 = metadata !{metadata !"ub41", i32 12, <4 x i1>* @ub41_typeProxy, metadata !2}
!6 = metadata !{metadata !"ub42", i32 12, <4 x i1>* @ub42_typeProxy, metadata !2}
!7 = metadata !{metadata !"uiv4", i32 12, <4 x i32>* @uiv4_typeProxy, metadata !2}
!8 = metadata !{metadata !"ub", i32 12, i1* @ub_typeProxy, metadata !2}
!9 = metadata !{metadata !"gl_FragColor", i32 7, <4 x float>* @gl_FragColor_typeProxy, metadata !10}
!10 = metadata !{i32 0, i32 0, i32 1024, null, i32 0}


Bottom IR:
; ModuleID = 'Glslang'

@uv4 = external addrspace(2) constant <4 x float>
@ui = external addrspace(2) constant i32
@uf = external addrspace(2) constant float
@ub41 = external addrspace(2) constant <4 x i1>
@ub42 = external addrspace(2) constant <4 x i1>
@gl_FragColor = global <4 x float> zeroinitializer

define fastcc void @main() {
entry:
  %0 = load <4 x float> addrspace(2)* @uv4, align 16, !gla.uniform !1
  %1 = fmul <4 x float> %0, <float 0x3F91DF46A0000000, float 0x3F91DF46A0000000, float 0x3F91DF46A0000000, float 0x3F91DF46A0000000>
  %2 = fmul <4 x float> %1, <float 0x404CA5DC20000000, float 0x404CA5DC20000000, float 0x404CA5DC20000000, float 0x404CA5DC20000000>
  %v2 = fadd <4 x float> %1, %2
  %3 = load i32 addrspace(2)* @ui, align 4, !gla.uniform !3
  %i3 = mul i32 %3, %3
  %4 = call <4 x float> @llvm.gla.fSin.v4f32.v4f32(<4 x float> %v2)
  %v4 = fadd <4 x float> %4, %v2
  %5 = call <4 x float> @llvm.gla.fCos.v4f32.v4f32(<4 x float> %v4)
  %v5 = fadd <4 x float> %5, %v4
  %6 = call <4 x float> @llvm.gla.fTan.v4f32.v4f32(<4 x float> %v5)
  %v6 = fadd <4 x float> %6, %v5
  %7 = call <4 x float> @llvm.gla.fAsin.v4f32.v4f32(<4 x float> %v6)
  %v7 = fadd <4 x float> %7, %v6
  %8 = call <4 x float> @llvm.gla.fAcos.v4f32.v4f32(<4 x float> %v7)
  %v8 = fadd <4 x float> %8, %v7
  %9 = call <4 x float> @llvm.gla.fAtan.v4f32.v4f32(<4 x float> %v8)
  %v9 = fadd <4 x float> %9, %v8
  %10 = call <4 x float> @llvm.gla.fSinh.v4f32.v4f32(<4 x float> %v9)
  %v10 = fadd <4 x float> %10, %v9
  %11 = call <4 x float> @llvm.gla.fCosh.v4f32.v4f32(<4 x float> %v10)
  %v11 = fadd <4 x float> %11, %v10
  %12 = call <4 x float> @llvm.gla.fTanh.v4f32.v4f32(<4 x float> %v11)
  %v12 = fadd <4 x float> %12, %v11
  %13 = call <4 x float> @llvm.gla.fAsinh.v4f32.v4f32(<4 x float> %v12)
  %v13 = fadd <4 x float> %13, %v12
  %14 = call <4 x float> @llvm.gla.fAcosh.v4f32.v4f32(<4 x float> %v13)
  %v14 = fadd <4 x float> %14, %v13
  %15 = call <4 x float> @llvm.gla.fAtanh.v4f32.v4f32(<4 x float> %v14)
  %v15 = fadd <4 x float> %15, %v14
  %16 = call <4 x float> @llvm.gla.fPow.v4f32.v4f32.v4f32(<4 x float> %v15, <4 x float> %v15)
  %v16 = fadd <4 x float> %16, %v15
  %17 = call <4 x float> @llvm.gla.fExp.v4f32.v4f32(<4 x float> %v16)
  %v17 = fadd <4 x float> %17, %v16
  %18 = call <4 x float> @llvm.gla.fLog.v4f32.v4f32(<4 x float> %v17)
  %v18 = fadd <4 x float> %18, %v17
  %19 = call <4 x float> @llvm.gla.fExp2.v4f32.v4f32(<4 x float> %v18)
  %v19 = fadd <4 x float> %19, %v18
  %20 = call <4 x float> @llvm.gla.fLog2.v4f32.v4f32(<4 x float> %v19)
  %v20 = fadd <4 x float> %20, %v19
  %21 = call <4 x float> @llvm.gla.fSqrt.v4f32.v4f32(<4 x float> %v20)
  %v21 = fadd <4 x float> %21, %v20
  %22 = call <4 x float> @llvm.gla.fInverseSqrt.v4f32.v4f32(<4 x float> %v21)
  %v22 = fadd <4 x float> %22, %v21
  %23 = call <4 x float> @llvm.gla.fAbs.v4f32.v4f32(<4 x float> %v22)
  %v23 = fadd <4 x float> %23, %v22
  %24 = call <4 x float> @llvm.gla.fSign.v4f32.v4f32(<4 x float> %v23)
  %v24 = fadd <4 x float> %24, %v23
  %25 = call <4 x float> @llvm.gla.fFloor.v4f32.v4f32(<4 x float> %v24)
  %v25 = fadd <4 x float> %25, %v24
  %26 = call <4 x float> @llvm.gla.fCeiling.v4f32.v4f32(<4 x float> %v25)
  %v26 = fadd <4 x float> %26, %v25
  %27 = call <4 x float> @llvm.gla.fFraction.v4f32.v4f32(<4 x float> %v26)
  %v27 = fadd <4 x float> %27, %v26
  %28 = frem <4 x float> %v27, %v27
  %v28 = fadd <4 x float> %28, %v27
  %29 = call <4 x float> @llvm.gla.fSwizzle.v4f32.v4f32.v4i32(<4 x float> %v28, <4 x i32> zeroinitializer)
  %30 = frem <4 x float> %v28, %29
  %v29 = fadd <4 x float> %30, %v28
  %31 = call <4 x float> @llvm.gla.fMin.v4f32.v4f32.v4f32(<4 x float> %v29, <4 x float> %0)
  %v30 = fadd <4 x float> %31, %v29
  %32 = call <4 x float> @llvm.gla.fMax.v4f32.v4f32.v4f32(<4 x float> %v30, <4 x float> %0)
  %v31 = fadd <4 x float> %32, %v30
  %33 = call <4 x float> @llvm.gla.fClamp.v4f32.v4f32.v4f32.v4f32(<4 x float> %v31, <4 x float> %0, <4 x float> %0)
  %v32 = fadd <4 x float> %33, %v31
  %34 = call <4 x float> @llvm.gla.fMix.v4f32.v4f32.v4f32.v4f32(<4 x float> %v32, <4 x float> %v32, <4 x float> %v32)
  %v33 = fadd <4 x float> %34, %v32
  %35 = call <4 x float> @llvm.gla.fStep.v4f32.v4f32.v4f32(<4 x float> %v33, <4 x float> %v33)
  %v34 = fadd <4 x float> %35, %v33
  %36 = call <4 x float> @llvm.gla.fSmoothStep.v4f32.v4f32.v4f32.v4f32(<4 x float> %v34, <4 x float> %v34, <4 x float> %v34)
  %v35 = fadd <4 x float> %36, %v34
  %37 = load float addrspace(2)* @uf, align 4, !gla.uniform !4
  %38 = call <4 x float> @llvm.gla.fStep.v4f32.f32.v4f32(float %37, <4 x float> %v35)
  %v36 = fadd <4 x float> %v35, %38
  %39 = call <4 x float> @llvm.gla.fSmoothStep.v4f32.f32.f32.v4f32(float %37, float %37, <4 x float> %v36)
  %v37 = fadd <4 x float> %v36, %39
  %40 = call <4 x float> @llvm.gla.fNormalize.v4f32.v4f32(<4 x float> %v37)
  %v38 = fadd <4 x float> %v37, %40
  %41 = call <4 x float> @llvm.gla.fFaceForward.v4f32.v4f32.v4f32.v4f32(<4 x float> %v38, <4 x float> %v38, <4 x float> %v38)
  %v39 = fadd <4 x float> %v38, %41
  %42 = call <4 x float> @llvm.gla.fReflect.v4f32.v4f32.v4f32(<4 x float> %v39, <4 x float> %v39)
  %v40 = fadd <4 x float> %v39, %42
  %43 = call <4 x float> @llvm.gla.fRefract.v4f32.v4f32.v4f32.f32(<4 x float> %v40, <4 x float> %v40, float %37)
  %v41 = fadd <4 x float> %v40, %43
  %44 = call <4 x float> @llvm.gla.fDFdx.v4f32.v4f32(<4 x float> %v41)
  %v42 = fadd <4 x float> %v41, %44
  %45 = call <4 x float> @llvm.gla.fDFdy.v4f32.v4f32(<4 x float> %v42)
  %v43 = fadd <4 x float> %v42, %45
  %46 = call <4 x float> @llvm.gla.fFilterWidth.v4f32.v4f32(<4 x float> %v43)
  %v44 = fadd <4 x float> %v43, %46
  %47 = call <3 x float> @llvm.gla.fSwizzle.v3f32.v4f32.v3i32(<4 x float> %v44, <3 x i32> <i32 0, i32 1, i32 2>)
  %48 = fcmp olt <4 x float> %v44, %0
  %b45 = call i1 @llvm.gla.any.v4i1(<4 x i1> %48)
  %49 = fcmp ole <4 x float> %v44, %0
  %50 = call i1 @llvm.gla.any.v4i1(<4 x i1> %49)
  %b46 = and i1 %b45, %50
  %51 = fcmp ogt <4 x float> %v44, %0
  %52 = call i1 @llvm.gla.any.v4i1(<4 x i1> %51)
  %b47 = and i1 %b46, %52
  %53 = fcmp oge <4 x float> %v44, %0
  %54 = call i1 @llvm.gla.any.v4i1(<4 x i1> %53)
  %b48 = and i1 %b47, %54
  %55 = load <4 x i1> addrspace(2)* @ub41, align 4, !gla.uniform !5
  %56 = load <4 x i1> addrspace(2)* @ub42, align 4, !gla.uniform !6
  %57 = icmp eq <4 x i1> %55, %56
  %58 = call i1 @llvm.gla.any.v4i1(<4 x i1> %57)
  %b49 = and i1 %b48, %58
  %59 = icmp ne <4 x i1> %55, %56
  %60 = call i1 @llvm.gla.any.v4i1(<4 x i1> %59)
  %b50 = and i1 %b49, %60
  %61 = call i1 @llvm.gla.any.v4i1(<4 x i1> %55)
  %b51 = and i1 %b50, %61
  %62 = call i1 @llvm.gla.all.v4i1(<4 x i1> %55)
  %b52 = and i1 %b51, %62
  %63 = xor <4 x i1> %55, <i1 true, i1 true, i1 true, i1 true>
  %64 = call i1 @llvm.gla.any.v4i1(<4 x i1> %63)
  %b53 = and i1 %b52, %64
  %65 = add i32 %i3, %3
  %66 = mul i32 %65, %i3
  %67 = sub i32 %66, %3
  %i54 = sdiv i32 %67, %i3
  %i55 = srem i32 %i54, %3
  %68 = icmp eq i32 %i55, 2
  %i56 = add i32 %i55, 1
  %.i56 = select i1 %68, i32 0, i32 %i56
  %69 = fadd float %37, %37
  %70 = fmul float %37, %69
  %71 = fsub float %70, %37
  %f57 = fdiv float %71, %37
  %72 = call float @llvm.gla.fLength.f32.v4f32(<4 x float> %v44)
  %f58 = fadd float %f57, %72
  %73 = call float @llvm.gla.fDot4.f32.v4f32.v4f32(<4 x float> %v44, <4 x float> %v44)
  %f60 = fadd float %f58, %73
  %74 = fmul float %37, %f60
  %f61 = fadd float %f60, %74
  %75 = call <3 x float> @llvm.gla.fCross.v3f32.v3f32.v3f32(<3 x float> %47, <3 x float> %47)
  %76 = extractelement <3 x float> %75, i32 0
  %f62 = fadd float %76, %f61
  %77 = fcmp oeq float %f62, %37
  %78 = fcmp one float %f62, %37
  %79 = fcmp one float %f62, 2.000000e+00
  %80 = and i1 %78, %79
  %81 = or i1 %77, %80
  %f64 = fadd float %f62, 1.000000e+00
  %select79 = select i1 %81, float %f64, float %f62
  %82 = call <4 x float> @llvm.gla.fSwizzle.v4f32.f32.v4i32(float %select79, <4 x i32> zeroinitializer)
  %83 = load i32 addrspace(2)* @ui, align 4, !gla.uniform !3
  %i66 = and i32 %83, %.i56
  %i67 = or i32 %i66, 66
  %i68 = xor i32 %i67, %83
  %i69 = srem i32 %i68, 17
  %i70 = ashr i32 %i69, 2
  %i71 = shl i32 %i70, %83
  %i72 = xor i32 %i71, -1
  %84 = sitofp i32 %i72 to float
  %85 = call <4 x float> @llvm.gla.fSwizzle.v4f32.f32.v4i32(float %84, <4 x i32> zeroinitializer)
  %86 = fadd <4 x float> %85, %82
  %ternary76 = fadd <4 x float> %v44, %86
  %select = select i1 %b53, <4 x float> %v44, <4 x float> %ternary76
  store <4 x float> %select, <4 x float>* @gl_FragColor, align 16
  br label %stage-epilogue

stage-epilogue:                                   ; preds = %entry
  br label %stage-exit

stage-exit:                                       ; preds = %stage-epilogue
  ret void
}

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSin.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fCos.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fTan.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAsin.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAcos.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAtan.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSinh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fCosh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fTanh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAsinh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAcosh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAtanh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fPow.v4f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fExp.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fLog.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fExp2.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fLog2.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSqrt.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fInverseSqrt.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAbs.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSign.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fFloor.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fCeiling.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fFraction.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fMin.v4f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fMax.v4f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fClamp.v4f32.v4f32.v4f32.v4f32(<4 x float>, <4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fMix.v4f32.v4f32.v4f32.v4f32(<4 x float>, <4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fStep.v4f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSmoothStep.v4f32.v4f32.v4f32.v4f32(<4 x float>, <4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fStep.v4f32.f32.v4f32(float, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSmoothStep.v4f32.f32.f32.v4f32(float, float, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fNormalize.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fFaceForward.v4f32.v4f32.v4f32.v4f32(<4 x float>, <4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fReflect.v4f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fRefract.v4f32.v4f32.v4f32.f32(<4 x float>, <4 x float>, float) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fDFdx.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fDFdy.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fFilterWidth.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare i1 @llvm.gla.any.v4i1(<4 x i1>) #0

; Function Attrs: nounwind readnone
declare i1 @llvm.gla.all.v4i1(<4 x i1>) #0

; Function Attrs: nounwind readnone
declare float @llvm.gla.fLength.f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare float @llvm.gla.fDot4.f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <3 x float> @llvm.gla.fCross.v3f32.v3f32.v3f32(<3 x float>, <3 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSwizzle.v4f32.v4f32.v4i32(<4 x float>, <4 x i32>) #0

; Function Attrs: nounwind readnone
declare <3 x float> @llvm.gla.fSwizzle.v3f32.v4f32.v3i32(<4 x float>, <3 x i32>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSwizzle.v4f32.f32.v4i32(float, <4 x i32>) #0

attributes #0 = { nounwind readnone }

!gla.entrypoint = !{!0}
!gla.uniforms = !{!1, !3, !4, !5, !6, !7, !8}
!gla.outputs = !{!9}
!gla.noStaticUse = !{!7, !8}

!0 = metadata !{metadata !"main", i32 15}
!1 = metadata !{metadata !"uv4", i32 12, <4 x float>* @uv4_typeProxy, metadata !2}
!2 = metadata !{i32 0, i32 0, i32 1024, null}
!3 = metadata !{metadata !"ui", i32 12, i32* @ui_typeProxy, metadata !2}
!4 = metadata !{metadata !"uf", i32 12, float* @uf_typeProxy, metadata !2}
!5 = metadata !{metadata !"ub41", i32 12, <4 x i1>* @ub41_typeProxy, metadata !2}
!6 = metadata !{metadata !"ub42", i32 12, <4 x i1>* @ub42_typeProxy, metadata !2}
!7 = metadata !{metadata !"uiv4", i32 12, <4 x i32>* @uiv4_typeProxy, metadata !2}
!8 = metadata !{metadata !"ub", i32 12, i1* @ub_typeProxy, metadata !2}
!9 = metadata !{metadata !"gl_FragColor", i32 7, <4 x float>* @gl_FragColor_typeProxy, metadata !10}
!10 = metadata !{i32 0, i32 0, i32 1024, null, i32 0}
#version 130
// LunarGOO output
uniform vec4 uv4;
uniform int ui;
uniform float uf;
uniform bvec4 ub41;
uniform bvec4 ub42;
uniform ivec4 uiv4;
uniform bool ub;
const vec4 C_vssely1 = vec4(0.0174533);
const vec4 C_4bmlfm = vec4(57.2958);
const int C_2 = 2;
const int C_1 = 1;
const int C_0 = 0;
const float C_2d0 = 2.0;
const float C_1d0 = 1.0;
const int C_66 = 66;
const int C_17 = 17;

void main()
{
	vec4 H_hgy73b1 = uv4 * C_vssely1;
	vec4 H_rrtr9n = H_hgy73b1 * C_4bmlfm;
	vec4 H_2u0bi2 = H_hgy73b1 + H_rrtr9n;
	int H_h8ktht1 = ui * ui;
	vec4 H_pr406p1 = sin(H_2u0bi2);
	vec4 H_1sil091 = H_2u0bi2 + H_pr406p1;
	vec4 H_pm9wt4 = cos(H_1sil091);
	vec4 H_60zb061 = H_1sil091 + H_pm9wt4;
	vec4 H_vqt3cx1 = tan(H_60zb061);
	vec4 H_b8e3h6 = H_60zb061 + H_vqt3cx1;
	vec4 H_mmsxys = asin(H_b8e3h6);
	vec4 H_fyum081 = H_b8e3h6 + H_mmsxys;
	vec4 H_xttkch = acos(H_fyum081);
	vec4 H_27h57a = H_fyum081 + H_xttkch;
	vec4 H_nzxhnp1 = atan(H_27h57a);
	vec4 H_jrwraf = H_27h57a + H_nzxhnp1;
	vec4 H_9ccpbm = sinh(H_jrwraf);
	vec4 H_iukhwg = H_9ccpbm + H_jrwraf;
	vec4 H_t95rkq1 = cosh(H_iukhwg);
	vec4 H_26o6hf = H_iukhwg + H_t95rkq1;
	vec4 H_9d78cj = tanh(H_26o6hf);
	vec4 H_49fyhm1 = H_26o6hf + H_9d78cj;
	vec4 H_sricuv1 = asinh(H_49fyhm1);
	vec4 H_bfg9ms1 = H_49fyhm1 + H_sricuv1;
	vec4 H_onqi3g1 = acosh(H_bfg9ms1);
	vec4 H_5yaqxs1 = H_bfg9ms1 + H_onqi3g1;
	vec4 H_lkv4ho1 = atanh(H_5yaqxs1);
	vec4 H_f8j5vq = H_5yaqxs1 + H_lkv4ho1;
	vec4 H_2mywoc = pow(H_f8j5vq, H_f8j5vq);
	vec4 H_tgc07q = H_2mywoc + H_f8j5vq;
	vec4 H_qpu2a11 = exp(H_tgc07q);
	vec4 H_dk4dcj = H_qpu2a11 + H_tgc07q;
	vec4 H_9xbaf = log(H_dk4dcj);
	vec4 H_svrpa71 = H_9xbaf + H_dk4dcj;
	vec4 H_ac2bc21 = exp2(H_svrpa71);
	vec4 H_katd4g1 = H_ac2bc21 + H_svrpa71;
	vec4 H_dsrnja = log2(H_katd4g1);
	vec4 H_2xmg6e = H_dsrnja + H_katd4g1;
	vec4 H_qojjv81 = sqrt(H_2xmg6e);
	vec4 H_823mgg = H_2xmg6e + H_qojjv81;
	vec4 H_lhpqhc1 = inversesqrt(H_823mgg);
	vec4 H_vu6cwi = H_823mgg + H_lhpqhc1;
	vec4 H_hybov3 = abs(H_vu6cwi);
	vec4 H_ls6j7z = H_hybov3 + H_vu6cwi;
	vec4 H_497jwu = sign(H_ls6j7z);
	vec4 H_8q4sa4 = H_497jwu + H_ls6j7z;
	vec4 H_4r9tqn1 = floor(H_8q4sa4);
	vec4 H_cq21og = H_4r9tqn1 + H_8q4sa4;
	vec4 H_pnd8en1 = ceil(H_cq21og);
	vec4 H_5ihndv1 = H_cq21og + H_pnd8en1;
	vec4 H_0fqsuy1 = fract(H_5ihndv1);
	vec4 H_0fwhsb1 = H_0fqsuy1 + H_5ihndv1;
	vec4 H_chemma = mod(H_0fwhsb1, H_0fwhsb1);
	vec4 H_4b0dru1 = H_0fwhsb1 + H_chemma;
	vec4 H_6twtmm = mod(H_4b0dru1, H_4b0dru1.xxxx);
	vec4 H_fc6qim = H_4b0dru1 + H_6twtmm;
	vec4 H_4ynrbx1 = min(H_fc6qim, uv4);
	vec4 H_qme0uc1 = H_4ynrbx1 + H_fc6qim;
	vec4 H_4mh53o1 = max(H_qme0uc1, uv4);
	vec4 H_d1n5jt = H_4mh53o1 + H_qme0uc1;
	vec4 H_2g9xs11 = clamp(H_d1n5jt, uv4, uv4);
	vec4 H_hnouuf = H_2g9xs11 + H_d1n5jt;
	vec4 H_iz8nxl = mix(H_hnouuf, H_hnouuf, H_hnouuf);
	vec4 H_o2jgoh = H_hnouuf + H_iz8nxl;
	vec4 H_gaxasp = step(H_o2jgoh, H_o2jgoh);
	vec4 H_3ou1m8 = H_gaxasp + H_o2jgoh;
	vec4 H_67nxre1 = smoothstep(H_3ou1m8, H_3ou1m8, H_3ou1m8);
	vec4 H_0t4vln1 = H_3ou1m8 + H_67nxre1;
	vec4 H_1vz1wq = step(uf, H_0t4vln1);
	vec4 H_xcgc5f1 = H_0t4vln1 + H_1vz1wq;
	vec4 H_o7jeak1 = smoothstep(uf, uf, H_xcgc5f1);
	vec4 H_b9n64q = H_o7jeak1 + H_xcgc5f1;
	vec4 H_q3slgi = normalize(H_b9n64q);
	vec4 H_3jo1tj1 = H_b9n64q + H_q3slgi;
	vec4 H_lrjeix1 = faceforward(H_3jo1tj1, H_3jo1tj1, H_3jo1tj1);
	vec4 H_tankrk = H_3jo1tj1 + H_lrjeix1;
	vec4 H_x6ffun1 = reflect(H_tankrk, H_tankrk);
	vec4 H_lwyi2i1 = H_tankrk + H_x6ffun1;
	vec4 H_uubbzb = refract(H_lwyi2i1, H_lwyi2i1, uf);
	vec4 H_bwor98 = H_lwyi2i1 + H_uubbzb;
	vec4 H_2802gi1 = dFdx(H_bwor98);
	vec4 H_cbjb581 = H_2802gi1 + H_bwor98;
	vec4 H_36po411 = dFdy(H_cbjb581);
	vec4 H_bn7jta = H_36po411 + H_cbjb581;
	vec4 H_tzraew = fwidth(H_bn7jta);
	vec4 H_fzc38q1 = H_bn7jta + H_tzraew;
	bvec4 H_gnurdt = lessThan(H_fzc38q1, uv4);
	bool H_jzivur1 = any(H_gnurdt);
	bvec4 H_qykp4o = lessThanEqual(H_fzc38q1, uv4);
	bool H_lxg04n1 = any(H_qykp4o);
	bool H_e65ujj1 = H_jzivur1 && H_lxg04n1;
	bvec4 H_po3x9e1 = greaterThan(H_fzc38q1, uv4);
	bool H_smlg6k = any(H_po3x9e1);
	bool H_72o0hj1 = H_e65ujj1 && H_smlg6k;
	bvec4 H_h7acak1 = greaterThanEqual(H_fzc38q1, uv4);
	bool H_xmtkou1 = any(H_h7acak1);
	bool H_rio2t = H_72o0hj1 && H_xmtkou1;
	bvec4 H_vlocan1 = equal(ub41, ub42);
	bool H_by51mk1 = any(H_vlocan1);
	bool H_dpz2rs = H_rio2t && H_by51mk1;
	bvec4 H_e6ku4t1 = notEqual(ub41, ub42);
	bool H_rjqzlx = any(H_e6ku4t1);
	bool H_1f85631 = H_dpz2rs && H_rjqzlx;
	bool H_u0wve91 = any(ub41);
	bool H_8lgm1k = H_1f85631 && H_u0wve91;
	bool H_f4b5uh1 = all(ub41);
	bool H_2t4w5q = H_8lgm1k && H_f4b5uh1;
	bvec4 H_db60wu = not(ub41);
	bool H_hpec3a1 = any(H_db60wu);
	bool H_3ewbjk = H_2t4w5q && H_hpec3a1;
	int H_ngz3vc1 = H_h8ktht1 + ui;
	int H_l1vuw31 = H_h8ktht1 * H_ngz3vc1;
	int H_2knscd = H_l1vuw31 - ui;
	int H_rq4ows = H_2knscd / H_h8ktht1;
	int H_xtc1vg = H_rq4ows % ui;
	bool H_ry3ksp = H_xtc1vg == C_2;
	int H_3j3nrk1 = H_xtc1vg + C_1;
	int _i_c1 = H_ry3ksp ? C_0 : H_3j3nrk1;
	float H_6ra9oe1 = uf + uf;
	float H_eccx591 = H_6ra9oe1 * uf;
	float H_my73qz = H_eccx591 - uf;
	float H_1umaut = H_my73qz / uf;
	float H_pebi7n1 = length(H_fzc38q1);
	float H_5868mb1 = H_1umaut + H_pebi7n1;
	float H_9xgw8z = dot(H_fzc38q1, H_fzc38q1);
	float H_ahac5a1 = H_5868mb1 + H_9xgw8z;
	float H_7kv9h31 = H_ahac5a1 * uf;
	float H_xz4r07 = H_7kv9h31 + H_ahac5a1;
	vec3 H_ku1yjn1 = cross(H_fzc38q1.xyz, H_fzc38q1.xyz);
	float H_yu4764 = H_ku1yjn1.x + H_xz4r07;
	bool H_kot1tw1 = H_yu4764 == uf;
	bool H_kzsh59 = H_yu4764 != uf;
	bool H_ppnz8y = H_yu4764 != C_2d0;
	bool H_qnzi2l = H_kzsh59 && H_ppnz8y;
	bool H_247rn71 = H_kot1tw1 || H_qnzi2l;
	float H_94a7zi1 = H_yu4764 + C_1d0;
	float select_c2 = H_247rn71 ? H_94a7zi1 : H_yu4764;
	vec4 H_27j1m6 = vec4(select_c2);
	int H_k4dj5f1 = ui & _i_c1;
	int H_9dkq3 = H_k4dj5f1 | C_66;
	int H_mg2t2h1 = H_9dkq3 ^ ui;
	int H_y407sw1 = H_mg2t2h1 % C_17;
	int H_v5by99 = H_y407sw1 >> C_2;
	int H_wtiyuj = H_v5by99 << ui;
	int H_j4o49c1 = ~(H_wtiyuj);
	float H_lplg6b1 = float(H_j4o49c1);
	vec4 H_nsrbiv = vec4(H_lplg6b1);
	vec4 H_xykq7k = H_27j1m6 + H_nsrbiv;
	vec4 ternary_c3 = H_fzc38q1 + H_xykq7k;
	vec4 select_c4 = H_3ewbjk ? H_fzc38q1 : ternary_c3;
	gl_FragColor = select_c4;
	
}

#version 130
// LunarGOO output
uniform vec4 uv4;
uniform int ui;
uniform float uf;
uniform bvec4 ub41;
uniform bvec4 ub42;
uniform ivec4 uiv4;
uniform bool ub;
const vec4 C_vssely1 = vec4(0.0174533);
const vec4 C_4bmlfm = vec4(57.2958);
const int C_2 = 2;
const int C_1 = 1;
const int C_0 = 0;
const float C_2d0 = 2.0;
const float C_1d0 = 1.0;
const int C_66 = 66;
const int C_17 = 17;

void main()
{
	vec4 H_hgy_c1 = uv4 * C_vssely1;
	vec4 H_rrtr_c2 = H_hgy_c1 * C_4bmlfm;
	vec4 H_uc3 = H_hgy_c1 + H_rrtr_c2;
	int H_h_c4 = ui * ui;
	vec4 H_pr_c5 = sin(H_uc3);
	vec4 H_uc6 = H_pr_c5 + H_uc3;
	vec4 H_pm_c7 = cos(H_uc6);
	vec4 H_uc8 = H_pm_c7 + H_uc6;
	vec4 H_vqt_c9 = tan(H_uc8);
	vec4 H_b_c10 = H_uc8 + H_vqt_c9;
	vec4 H_mmsxys_c11 = asin(H_b_c10);
	vec4 H_fyum_c12 = H_b_c10 + H_mmsxys_c11;
	vec4 H_xttkch_c13 = acos(H_fyum_c12);
	vec4 H_uc14 = H_fyum_c12 + H_xttkch_c13;
	vec4 H_nzxhnp_c15 = atan(H_uc14);
	vec4 H_jrwraf_c16 = H_nzxhnp_c15 + H_uc14;
	vec4 H_uc17 = sinh(H_jrwraf_c16);
	vec4 H_iukhwg_c18 = H_jrwraf_c16 + H_uc17;
	vec4 H_t_c19 = cosh(H_iukhwg_c18);
	vec4 H_uc20 = H_iukhwg_c18 + H_t_c19;
	vec4 H_uc21 = tanh(H_uc20);
	vec4 H_uc22 = H_uc20 + H_uc21;
	vec4 H_sricuv_c23 = asinh(H_uc22);
	vec4 H_bfg_c24 = H_sricuv_c23 + H_uc22;
	vec4 H_onqi_c25 = acosh(H_bfg_c24);
	vec4 H_uc26 = H_bfg_c24 + H_onqi_c25;
	vec4 H_lkv_c27 = atanh(H_uc26);
	vec4 H_f_c28 = H_lkv_c27 + H_uc26;
	vec4 H_uc29 = pow(H_f_c28, H_f_c28);
	vec4 H_tgc_c30 = H_f_c28 + H_uc29;
	vec4 H_qpu_c31 = exp(H_tgc_c30);
	vec4 H_dk_c32 = H_qpu_c31 + H_tgc_c30;
	vec4 H_uc33 = log(H_dk_c32);
	vec4 H_svrpa_c34 = H_dk_c32 + H_uc33;
	vec4 H_ac_c35 = exp2(H_svrpa_c34);
	vec4 H_katd_c36 = H_ac_c35 + H_svrpa_c34;
	vec4 H_dsrnja_c37 = log2(H_katd_c36);
	vec4 H_uc38 = H_dsrnja_c37 + H_katd_c36;
	vec4 H_qojjv_c39 = sqrt(H_uc38);
	vec4 H_uc40 = H_qojjv_c39 + H_uc38;
	vec4 H_lhpqhc_c41 = inversesqrt(H_uc40);
	vec4 H_vu_c42 = H_lhpqhc_c41 + H_uc40;
	vec4 H_hybov_c43 = abs(H_vu_c42);
	vec4 H_ls_c44 = H_hybov_c43 + H_vu_c42;
	vec4 H_uc45 = sign(H_ls_c44);
	vec4 H_uc46 = H_ls_c44 + H_uc45;
	vec4 H_uc47 = floor(H_uc46);
	vec4 H_cq_c48 = H_uc46 + H_uc47;
	vec4 H_pnd_c49 = ceil(H_cq_c48);
	vec4 H_uc50 = H_cq_c48 + H_pnd_c49;
	vec4 H_uc51 = fract(H_uc50);
	vec4 H_uc52 = H_uc50 + H_uc51;
	vec4 H_chemma_c53 = mod(H_uc52, H_uc52);
	vec4 H_uc54 = H_chemma_c53 + H_uc52;
	vec4 H_uc55 = mod(H_uc54, H_uc54.xxxx);
	vec4 H_fc_c56 = H_uc54 + H_uc55;
	vec4 H_uc57 = min(H_fc_c56, uv4);
	vec4 H_qme_c58 = H_fc_c56 + H_uc57;
	vec4 H_uc59 = max(H_qme_c58, uv4);
	vec4 H_d_c60 = H_qme_c58 + H_uc59;
	vec4 H_uc61 = clamp(H_d_c60, uv4, uv4);
	vec4 H_hnouuf_c62 = H_d_c60 + H_uc61;
	vec4 H_iz_c63 = mix(H_hnouuf_c62, H_hnouuf_c62, H_hnouuf_c62);
	vec4 H_o_c64 = H_hnouuf_c62 + H_iz_c63;
	vec4 H_gaxasp_c65 = step(H_o_c64, H_o_c64);
	vec4 H_uc66 = H_gaxasp_c65 + H_o_c64;
	vec4 H_uc67 = smoothstep(H_uc66, H_uc66, H_uc66);
	vec4 H_uc68 = H_uc66 + H_uc67;
	vec4 H_uc69 = step(uf, H_uc68);
	vec4 H_xcgc_c70 = H_uc68 + H_uc69;
	vec4 H_o_c71 = smoothstep(uf, uf, H_xcgc_c70);
	vec4 H_b_c72 = H_o_c71 + H_xcgc_c70;
	vec4 H_q_c73 = normalize(H_b_c72);
	vec4 H_uc74 = H_b_c72 + H_q_c73;
	vec4 H_lrjeix_c75 = faceforward(H_uc74, H_uc74, H_uc74);
	vec4 H_tankrk_c76 = H_lrjeix_c75 + H_uc74;
	vec4 H_x_c77 = reflect(H_tankrk_c76, H_tankrk_c76);
	vec4 H_lwyi_c78 = H_tankrk_c76 + H_x_c77;
	vec4 H_uubbzb_c79 = refract(H_lwyi_c78, H_lwyi_c78, uf);
	vec4 H_bwor_c80 = H_lwyi_c78 + H_uubbzb_c79;
	vec4 H_uc81 = dFdx(H_bwor_c80);
	vec4 H_cbjb_c82 = H_bwor_c80 + H_uc81;
	vec4 H_uc83 = dFdy(H_cbjb_c82);
	vec4 H_bn_c84 = H_cbjb_c82 + H_uc83;
	vec4 H_tzraew_c85 = fwidth(H_bn_c84);
	vec4 H_fzc_c86 = H_bn_c84 + H_tzraew_c85;
	bvec4 H_gnurdt_c87 = lessThan(H_fzc_c86, uv4);
	bool H_jzivur_c88 = any(H_gnurdt_c87);
	bvec4 H_qykp_c89 = lessThanEqual(H_fzc_c86, uv4);
	bool H_lxg_c90 = any(H_qykp_c89);
	bool H_e_c91 = H_jzivur_c88 && H_lxg_c90;
	bvec4 H_po_c92 = greaterThan(H_fzc_c86, uv4);
	bool H_smlg_c93 = any(H_po_c92);
	bool H_uc94 = H_e_c91 && H_smlg_c93;
	bvec4 H_h_c95 = greaterThanEqual(H_fzc_c86, uv4);
	bool H_xmtkou_c96 = any(H_h_c95);
	bool H_rio_c97 = H_uc94 && H_xmtkou_c96;
	bvec4 H_vlocan_c98 = equal(ub41, ub42);
	bool H_by_c99 = any(H_vlocan_c98);
	bool H_dpz_c100 = H_rio_c97 && H_by_c99;
	bvec4 H_e_c101 = notEqual(ub41, ub42);
	bool H_rjqzlx_c102 = any(H_e_c101);
	bool H_uc103 = H_dpz_c100 && H_rjqzlx_c102;
	bool H_u_c104 = any(ub41);
	bool H_uc105 = H_uc103 && H_u_c104;
	bool H_f_c106 = all(ub41);
	bool H_uc107 = H_uc105 && H_f_c106;
	bvec4 H_db_c108 = not(ub41);
	bool H_hpec_c109 = any(H_db_c108);
	bool H_uc110 = H_uc107 && H_hpec_c109;
	int H_ngz_c111 = H_h_c4 + ui;
	int H_l_c112 = H_h_c4 * H_ngz_c111;
	int H_uc113 = H_l_c112 - ui;
	int H_rq_c114 = H_uc113 / H_h_c4;
	int H_xtc_c115 = H_rq_c114 % ui;
	bool H_ry_c116 = H_xtc_c115 == C_2;
	int H_uc117 = H_xtc_c115 + C_1;
	int ternary_uc118 = H_ry_c116 ? C_0 : H_uc117;
	float H_uc119 = uf + uf;
	float H_eccx_c120 = H_uc119 * uf;
	float H_my_c121 = H_eccx_c120 - uf;
	float H_uc122 = H_my_c121 / uf;
	float H_pebi_c123 = length(H_fzc_c86);
	float H_uc124 = H_pebi_c123 + H_uc122;
	float H_uc125 = dot(H_fzc_c86, H_fzc_c86);
	float H_ahac_c126 = H_uc124 + H_uc125;
	float H_uc127 = H_ahac_c126 * uf;
	float H_xz_c128 = H_ahac_c126 + H_uc127;
	vec3 H_ku_c129 = cross(H_fzc_c86.xyz, H_fzc_c86.xyz);
	float H_yu_c130 = H_ku_c129.x + H_xz_c128;
	bool H_kot_c131 = H_yu_c130 == uf;
	bool H_kzsh_c132 = H_yu_c130 != uf;
	bool H_ppnz_c133 = H_yu_c130 != C_2d0;
	bool H_qnzi_c134 = H_kzsh_c132 && H_ppnz_c133;
	bool H_uc135 = H_kot_c131 || H_qnzi_c134;
	float H_uc136 = H_yu_c130 + C_1d0;
	float select_c137 = H_uc135 ? H_uc136 : H_yu_c130;
	vec4 H_3gnewf1 = vec4(select_c137);
	int H_k_c138 = ui & ternary_uc118;
	int H_uc139 = H_k_c138 | C_66;
	int H_mg_c140 = H_uc139 ^ ui;
	int H_y_c141 = H_mg_c140 % C_17;
	int H_v_c142 = H_y_c141 >> C_2;
	int H_wtiyuj_c143 = H_v_c142 << ui;
	int H_j_c144 = ~(H_wtiyuj_c143);
	float H_lplg_c145 = float(H_j_c144);
	vec4 H_k78ctw = vec4(H_lplg_c145);
	vec4 H_xykq_c146 = H_3gnewf1 + H_k78ctw;
	vec4 ternary_c147 = H_fzc_c86 + H_xykq_c146;
	vec4 select_c148 = H_uc110 ? H_fzc_c86 : ternary_c147;
	gl_FragColor = select_c148;
	
}

