
Top IR:
; ModuleID = 'Glslang'

@uv4 = external addrspace(2) constant <4 x float>
@ui = external addrspace(2) constant i32
@uf = external addrspace(2) constant float
@ub41 = external addrspace(2) constant <4 x i1>
@ub42 = external addrspace(2) constant <4 x i1>
@gl_FragColor = global <4 x float> zeroinitializer
@uiv4 = external addrspace(2) constant <4 x i32>
@ub = external addrspace(2) constant i1

define fastcc void @main() {
entry:
  %constructed75 = alloca <4 x float>
  %constructed = alloca <4 x float>
  %ternary = alloca <4 x float>
  %f = alloca float
  %b = alloca i1
  %i = alloca i32
  %v = alloca <4 x float>
  %0 = load <4 x float> addrspace(2)* @uv4, !gla.uniform !1
  %v1 = call <4 x float> @llvm.gla.fRadians.v4f32.v4f32(<4 x float> %0)
  store <4 x float> %v1, <4 x float>* %v
  %1 = load <4 x float>* %v
  %2 = call <4 x float> @llvm.gla.fDegrees.v4f32.v4f32(<4 x float> %1)
  %3 = load <4 x float>* %v
  %v2 = fadd <4 x float> %3, %2
  store <4 x float> %v2, <4 x float>* %v
  %4 = load i32 addrspace(2)* @ui, !gla.uniform !3
  %5 = load i32 addrspace(2)* @ui, !gla.uniform !3
  %i3 = mul i32 %4, %5
  store i32 %i3, i32* %i
  %6 = load <4 x float>* %v
  %7 = call <4 x float> @llvm.gla.fSin.v4f32.v4f32(<4 x float> %6)
  %8 = load <4 x float>* %v
  %v4 = fadd <4 x float> %8, %7
  store <4 x float> %v4, <4 x float>* %v
  %9 = load <4 x float>* %v
  %10 = call <4 x float> @llvm.gla.fCos.v4f32.v4f32(<4 x float> %9)
  %11 = load <4 x float>* %v
  %v5 = fadd <4 x float> %11, %10
  store <4 x float> %v5, <4 x float>* %v
  %12 = load <4 x float>* %v
  %13 = call <4 x float> @llvm.gla.fTan.v4f32.v4f32(<4 x float> %12)
  %14 = load <4 x float>* %v
  %v6 = fadd <4 x float> %14, %13
  store <4 x float> %v6, <4 x float>* %v
  %15 = load <4 x float>* %v
  %16 = call <4 x float> @llvm.gla.fAsin.v4f32.v4f32(<4 x float> %15)
  %17 = load <4 x float>* %v
  %v7 = fadd <4 x float> %17, %16
  store <4 x float> %v7, <4 x float>* %v
  %18 = load <4 x float>* %v
  %19 = call <4 x float> @llvm.gla.fAcos.v4f32.v4f32(<4 x float> %18)
  %20 = load <4 x float>* %v
  %v8 = fadd <4 x float> %20, %19
  store <4 x float> %v8, <4 x float>* %v
  %21 = load <4 x float>* %v
  %22 = call <4 x float> @llvm.gla.fAtan.v4f32.v4f32(<4 x float> %21)
  %23 = load <4 x float>* %v
  %v9 = fadd <4 x float> %23, %22
  store <4 x float> %v9, <4 x float>* %v
  %24 = load <4 x float>* %v
  %25 = call <4 x float> @llvm.gla.fSinh.v4f32.v4f32(<4 x float> %24)
  %26 = load <4 x float>* %v
  %v10 = fadd <4 x float> %26, %25
  store <4 x float> %v10, <4 x float>* %v
  %27 = load <4 x float>* %v
  %28 = call <4 x float> @llvm.gla.fCosh.v4f32.v4f32(<4 x float> %27)
  %29 = load <4 x float>* %v
  %v11 = fadd <4 x float> %29, %28
  store <4 x float> %v11, <4 x float>* %v
  %30 = load <4 x float>* %v
  %31 = call <4 x float> @llvm.gla.fTanh.v4f32.v4f32(<4 x float> %30)
  %32 = load <4 x float>* %v
  %v12 = fadd <4 x float> %32, %31
  store <4 x float> %v12, <4 x float>* %v
  %33 = load <4 x float>* %v
  %34 = call <4 x float> @llvm.gla.fAsinh.v4f32.v4f32(<4 x float> %33)
  %35 = load <4 x float>* %v
  %v13 = fadd <4 x float> %35, %34
  store <4 x float> %v13, <4 x float>* %v
  %36 = load <4 x float>* %v
  %37 = call <4 x float> @llvm.gla.fAcosh.v4f32.v4f32(<4 x float> %36)
  %38 = load <4 x float>* %v
  %v14 = fadd <4 x float> %38, %37
  store <4 x float> %v14, <4 x float>* %v
  %39 = load <4 x float>* %v
  %40 = call <4 x float> @llvm.gla.fAtanh.v4f32.v4f32(<4 x float> %39)
  %41 = load <4 x float>* %v
  %v15 = fadd <4 x float> %41, %40
  store <4 x float> %v15, <4 x float>* %v
  %42 = load <4 x float>* %v
  %43 = load <4 x float>* %v
  %44 = call <4 x float> @llvm.gla.fPow.v4f32.v4f32.v4f32(<4 x float> %42, <4 x float> %43)
  %45 = load <4 x float>* %v
  %v16 = fadd <4 x float> %45, %44
  store <4 x float> %v16, <4 x float>* %v
  %46 = load <4 x float>* %v
  %47 = call <4 x float> @llvm.gla.fExp.v4f32.v4f32(<4 x float> %46)
  %48 = load <4 x float>* %v
  %v17 = fadd <4 x float> %48, %47
  store <4 x float> %v17, <4 x float>* %v
  %49 = load <4 x float>* %v
  %50 = call <4 x float> @llvm.gla.fLog.v4f32.v4f32(<4 x float> %49)
  %51 = load <4 x float>* %v
  %v18 = fadd <4 x float> %51, %50
  store <4 x float> %v18, <4 x float>* %v
  %52 = load <4 x float>* %v
  %53 = call <4 x float> @llvm.gla.fExp2.v4f32.v4f32(<4 x float> %52)
  %54 = load <4 x float>* %v
  %v19 = fadd <4 x float> %54, %53
  store <4 x float> %v19, <4 x float>* %v
  %55 = load <4 x float>* %v
  %56 = call <4 x float> @llvm.gla.fLog2.v4f32.v4f32(<4 x float> %55)
  %57 = load <4 x float>* %v
  %v20 = fadd <4 x float> %57, %56
  store <4 x float> %v20, <4 x float>* %v
  %58 = load <4 x float>* %v
  %59 = call <4 x float> @llvm.gla.fSqrt.v4f32.v4f32(<4 x float> %58)
  %60 = load <4 x float>* %v
  %v21 = fadd <4 x float> %60, %59
  store <4 x float> %v21, <4 x float>* %v
  %61 = load <4 x float>* %v
  %62 = call <4 x float> @llvm.gla.fInverseSqrt.v4f32.v4f32(<4 x float> %61)
  %63 = load <4 x float>* %v
  %v22 = fadd <4 x float> %63, %62
  store <4 x float> %v22, <4 x float>* %v
  %64 = load <4 x float>* %v
  %65 = call <4 x float> @llvm.gla.fAbs.v4f32.v4f32(<4 x float> %64)
  %66 = load <4 x float>* %v
  %v23 = fadd <4 x float> %66, %65
  store <4 x float> %v23, <4 x float>* %v
  %67 = load <4 x float>* %v
  %68 = call <4 x float> @llvm.gla.fSign.v4f32.v4f32(<4 x float> %67)
  %69 = load <4 x float>* %v
  %v24 = fadd <4 x float> %69, %68
  store <4 x float> %v24, <4 x float>* %v
  %70 = load <4 x float>* %v
  %71 = call <4 x float> @llvm.gla.fFloor.v4f32.v4f32(<4 x float> %70)
  %72 = load <4 x float>* %v
  %v25 = fadd <4 x float> %72, %71
  store <4 x float> %v25, <4 x float>* %v
  %73 = load <4 x float>* %v
  %74 = call <4 x float> @llvm.gla.fCeiling.v4f32.v4f32(<4 x float> %73)
  %75 = load <4 x float>* %v
  %v26 = fadd <4 x float> %75, %74
  store <4 x float> %v26, <4 x float>* %v
  %76 = load <4 x float>* %v
  %77 = call <4 x float> @llvm.gla.fFraction.v4f32.v4f32(<4 x float> %76)
  %78 = load <4 x float>* %v
  %v27 = fadd <4 x float> %78, %77
  store <4 x float> %v27, <4 x float>* %v
  %79 = load <4 x float>* %v
  %80 = load <4 x float>* %v
  %81 = frem <4 x float> %79, %80
  %82 = load <4 x float>* %v
  %v28 = fadd <4 x float> %82, %81
  store <4 x float> %v28, <4 x float>* %v
  %83 = load <4 x float>* %v
  %84 = load <4 x float>* %v
  %85 = extractelement <4 x float> %84, i32 0
  %86 = insertelement <4 x float> undef, float %85, i32 0
  %87 = insertelement <4 x float> %86, float %85, i32 1
  %88 = insertelement <4 x float> %87, float %85, i32 2
  %89 = insertelement <4 x float> %88, float %85, i32 3
  %90 = frem <4 x float> %83, %89
  %91 = load <4 x float>* %v
  %v29 = fadd <4 x float> %91, %90
  store <4 x float> %v29, <4 x float>* %v
  %92 = load <4 x float>* %v
  %93 = load <4 x float> addrspace(2)* @uv4, !gla.uniform !1
  %94 = call <4 x float> @llvm.gla.fMin.v4f32.v4f32.v4f32(<4 x float> %92, <4 x float> %93)
  %95 = load <4 x float>* %v
  %v30 = fadd <4 x float> %95, %94
  store <4 x float> %v30, <4 x float>* %v
  %96 = load <4 x float>* %v
  %97 = load <4 x float> addrspace(2)* @uv4, !gla.uniform !1
  %98 = call <4 x float> @llvm.gla.fMax.v4f32.v4f32.v4f32(<4 x float> %96, <4 x float> %97)
  %99 = load <4 x float>* %v
  %v31 = fadd <4 x float> %99, %98
  store <4 x float> %v31, <4 x float>* %v
  %100 = load <4 x float>* %v
  %101 = load <4 x float> addrspace(2)* @uv4, !gla.uniform !1
  %102 = load <4 x float> addrspace(2)* @uv4, !gla.uniform !1
  %103 = call <4 x float> @llvm.gla.fClamp.v4f32.v4f32.v4f32.v4f32(<4 x float> %100, <4 x float> %101, <4 x float> %102)
  %104 = load <4 x float>* %v
  %v32 = fadd <4 x float> %104, %103
  store <4 x float> %v32, <4 x float>* %v
  %105 = load <4 x float>* %v
  %106 = load <4 x float>* %v
  %107 = load <4 x float>* %v
  %108 = call <4 x float> @llvm.gla.fMix.v4f32.v4f32.v4f32.v4f32(<4 x float> %105, <4 x float> %106, <4 x float> %107)
  %109 = load <4 x float>* %v
  %v33 = fadd <4 x float> %109, %108
  store <4 x float> %v33, <4 x float>* %v
  %110 = load <4 x float>* %v
  %111 = load <4 x float>* %v
  %112 = call <4 x float> @llvm.gla.fStep.v4f32.v4f32.v4f32(<4 x float> %110, <4 x float> %111)
  %113 = load <4 x float>* %v
  %v34 = fadd <4 x float> %113, %112
  store <4 x float> %v34, <4 x float>* %v
  %114 = load <4 x float>* %v
  %115 = load <4 x float>* %v
  %116 = load <4 x float>* %v
  %117 = call <4 x float> @llvm.gla.fSmoothStep.v4f32.v4f32.v4f32.v4f32(<4 x float> %114, <4 x float> %115, <4 x float> %116)
  %118 = load <4 x float>* %v
  %v35 = fadd <4 x float> %118, %117
  store <4 x float> %v35, <4 x float>* %v
  %119 = load float addrspace(2)* @uf, !gla.uniform !4
  %120 = load <4 x float>* %v
  %121 = call <4 x float> @llvm.gla.fStep.v4f32.f32.v4f32(float %119, <4 x float> %120)
  %122 = load <4 x float>* %v
  %v36 = fadd <4 x float> %122, %121
  store <4 x float> %v36, <4 x float>* %v
  %123 = load float addrspace(2)* @uf, !gla.uniform !4
  %124 = load float addrspace(2)* @uf, !gla.uniform !4
  %125 = load <4 x float>* %v
  %126 = call <4 x float> @llvm.gla.fSmoothStep.v4f32.f32.f32.v4f32(float %123, float %124, <4 x float> %125)
  %127 = load <4 x float>* %v
  %v37 = fadd <4 x float> %127, %126
  store <4 x float> %v37, <4 x float>* %v
  %128 = load <4 x float>* %v
  %129 = call <4 x float> @llvm.gla.fNormalize.v4f32.v4f32(<4 x float> %128)
  %130 = load <4 x float>* %v
  %v38 = fadd <4 x float> %130, %129
  store <4 x float> %v38, <4 x float>* %v
  %131 = load <4 x float>* %v
  %132 = load <4 x float>* %v
  %133 = load <4 x float>* %v
  %134 = call <4 x float> @llvm.gla.fFaceForward.v4f32.v4f32.v4f32.v4f32(<4 x float> %131, <4 x float> %132, <4 x float> %133)
  %135 = load <4 x float>* %v
  %v39 = fadd <4 x float> %135, %134
  store <4 x float> %v39, <4 x float>* %v
  %136 = load <4 x float>* %v
  %137 = load <4 x float>* %v
  %138 = call <4 x float> @llvm.gla.fReflect.v4f32.v4f32.v4f32(<4 x float> %136, <4 x float> %137)
  %139 = load <4 x float>* %v
  %v40 = fadd <4 x float> %139, %138
  store <4 x float> %v40, <4 x float>* %v
  %140 = load <4 x float>* %v
  %141 = load <4 x float>* %v
  %142 = load float addrspace(2)* @uf, !gla.uniform !4
  %143 = call <4 x float> @llvm.gla.fRefract.v4f32.v4f32.v4f32.f32(<4 x float> %140, <4 x float> %141, float %142)
  %144 = load <4 x float>* %v
  %v41 = fadd <4 x float> %144, %143
  store <4 x float> %v41, <4 x float>* %v
  %145 = load <4 x float>* %v
  %146 = call <4 x float> @llvm.gla.fDFdx.v4f32.v4f32(<4 x float> %145)
  %147 = load <4 x float>* %v
  %v42 = fadd <4 x float> %147, %146
  store <4 x float> %v42, <4 x float>* %v
  %148 = load <4 x float>* %v
  %149 = call <4 x float> @llvm.gla.fDFdy.v4f32.v4f32(<4 x float> %148)
  %150 = load <4 x float>* %v
  %v43 = fadd <4 x float> %150, %149
  store <4 x float> %v43, <4 x float>* %v
  %151 = load <4 x float>* %v
  %152 = call <4 x float> @llvm.gla.fFilterWidth.v4f32.v4f32(<4 x float> %151)
  %153 = load <4 x float>* %v
  %v44 = fadd <4 x float> %153, %152
  store <4 x float> %v44, <4 x float>* %v
  %154 = load <4 x float>* %v
  %155 = load <4 x float> addrspace(2)* @uv4, !gla.uniform !1
  %156 = fcmp olt <4 x float> %154, %155
  %b45 = call i1 @llvm.gla.any.v4i1(<4 x i1> %156)
  store i1 %b45, i1* %b
  %157 = load i1* %b
  %158 = load <4 x float>* %v
  %159 = load <4 x float> addrspace(2)* @uv4, !gla.uniform !1
  %160 = fcmp ole <4 x float> %158, %159
  %161 = call i1 @llvm.gla.any.v4i1(<4 x i1> %160)
  %b46 = and i1 %157, %161
  store i1 %b46, i1* %b
  %162 = load i1* %b
  %163 = load <4 x float>* %v
  %164 = load <4 x float> addrspace(2)* @uv4, !gla.uniform !1
  %165 = fcmp ogt <4 x float> %163, %164
  %166 = call i1 @llvm.gla.any.v4i1(<4 x i1> %165)
  %b47 = and i1 %162, %166
  store i1 %b47, i1* %b
  %167 = load i1* %b
  %168 = load <4 x float>* %v
  %169 = load <4 x float> addrspace(2)* @uv4, !gla.uniform !1
  %170 = fcmp oge <4 x float> %168, %169
  %171 = call i1 @llvm.gla.any.v4i1(<4 x i1> %170)
  %b48 = and i1 %167, %171
  store i1 %b48, i1* %b
  %172 = load i1* %b
  %173 = load <4 x i1> addrspace(2)* @ub41, !gla.uniform !5
  %174 = load <4 x i1> addrspace(2)* @ub42, !gla.uniform !6
  %175 = icmp eq <4 x i1> %173, %174
  %176 = call i1 @llvm.gla.any.v4i1(<4 x i1> %175)
  %b49 = and i1 %172, %176
  store i1 %b49, i1* %b
  %177 = load i1* %b
  %178 = load <4 x i1> addrspace(2)* @ub41, !gla.uniform !5
  %179 = load <4 x i1> addrspace(2)* @ub42, !gla.uniform !6
  %180 = icmp ne <4 x i1> %178, %179
  %181 = call i1 @llvm.gla.any.v4i1(<4 x i1> %180)
  %b50 = and i1 %177, %181
  store i1 %b50, i1* %b
  %182 = load i1* %b
  %183 = load <4 x i1> addrspace(2)* @ub41, !gla.uniform !5
  %184 = call i1 @llvm.gla.any.v4i1(<4 x i1> %183)
  %b51 = and i1 %182, %184
  store i1 %b51, i1* %b
  %185 = load i1* %b
  %186 = load <4 x i1> addrspace(2)* @ub41, !gla.uniform !5
  %187 = call i1 @llvm.gla.all.v4i1(<4 x i1> %186)
  %b52 = and i1 %185, %187
  store i1 %b52, i1* %b
  %188 = load i1* %b
  %189 = load <4 x i1> addrspace(2)* @ub41, !gla.uniform !5
  %190 = xor <4 x i1> %189, <i1 true, i1 true, i1 true, i1 true>
  %191 = call i1 @llvm.gla.any.v4i1(<4 x i1> %190)
  %b53 = and i1 %188, %191
  store i1 %b53, i1* %b
  %192 = load i32* %i
  %193 = load i32 addrspace(2)* @ui, !gla.uniform !3
  %194 = add i32 %192, %193
  %195 = load i32* %i
  %196 = mul i32 %194, %195
  %197 = load i32 addrspace(2)* @ui, !gla.uniform !3
  %198 = sub i32 %196, %197
  %199 = load i32* %i
  %i54 = sdiv i32 %198, %199
  store i32 %i54, i32* %i
  %200 = load i32* %i
  %201 = load i32 addrspace(2)* @ui, !gla.uniform !3
  %i55 = srem i32 %200, %201
  store i32 %i55, i32* %i
  %202 = load i32* %i
  %203 = load i32 addrspace(2)* @ui, !gla.uniform !3
  %204 = icmp eq i32 %202, %203
  %205 = load i32* %i
  %206 = load i32 addrspace(2)* @ui, !gla.uniform !3
  %207 = icmp ne i32 %205, %206
  %208 = load i32* %i
  %209 = load i32 addrspace(2)* @ui, !gla.uniform !3
  %210 = icmp eq i32 %208, %209
  %211 = and i1 %207, %210
  %212 = load i32* %i
  %213 = icmp ne i32 %212, 2
  %214 = xor i1 %211, %213
  %215 = or i1 %204, %214
  br i1 %215, label %then, label %ifmerge

then:                                             ; preds = %entry
  %216 = load i32* %i
  %i56 = add i32 %216, 1
  store i32 %i56, i32* %i
  br label %ifmerge

ifmerge:                                          ; preds = %entry, %then
  %217 = load float addrspace(2)* @uf, !gla.uniform !4
  %218 = load float addrspace(2)* @uf, !gla.uniform !4
  %219 = fadd float %217, %218
  %220 = load float addrspace(2)* @uf, !gla.uniform !4
  %221 = fmul float %219, %220
  %222 = load float addrspace(2)* @uf, !gla.uniform !4
  %223 = fsub float %221, %222
  %224 = load float addrspace(2)* @uf, !gla.uniform !4
  %f57 = fdiv float %223, %224
  store float %f57, float* %f
  %225 = load <4 x float>* %v
  %226 = call float @llvm.gla.fLength.f32.v4f32(<4 x float> %225)
  %227 = load float* %f
  %f58 = fadd float %227, %226
  store float %f58, float* %f
  %228 = load <4 x float>* %v
  %229 = load <4 x float>* %v
  %230 = call float @llvm.gla.fDistance.f32.v4f32.v4f32(<4 x float> %228, <4 x float> %229)
  %231 = load float* %f
  %f59 = fadd float %231, %230
  store float %f59, float* %f
  %232 = load <4 x float>* %v
  %233 = load <4 x float>* %v
  %234 = call float @llvm.gla.fDot4.f32.v4f32.v4f32(<4 x float> %232, <4 x float> %233)
  %235 = load float* %f
  %f60 = fadd float %235, %234
  store float %f60, float* %f
  %236 = load float* %f
  %237 = load float addrspace(2)* @uf, !gla.uniform !4
  %238 = fmul float %236, %237
  %239 = load float* %f
  %f61 = fadd float %239, %238
  store float %f61, float* %f
  %240 = load <4 x float>* %v
  %241 = extractelement <4 x float> %240, i32 0
  %242 = insertelement <3 x float> undef, float %241, i32 0
  %243 = extractelement <4 x float> %240, i32 1
  %244 = insertelement <3 x float> %242, float %243, i32 1
  %245 = extractelement <4 x float> %240, i32 2
  %246 = insertelement <3 x float> %244, float %245, i32 2
  %247 = load <4 x float>* %v
  %248 = extractelement <4 x float> %247, i32 0
  %249 = insertelement <3 x float> undef, float %248, i32 0
  %250 = extractelement <4 x float> %247, i32 1
  %251 = insertelement <3 x float> %249, float %250, i32 1
  %252 = extractelement <4 x float> %247, i32 2
  %253 = insertelement <3 x float> %251, float %252, i32 2
  %254 = call <3 x float> @llvm.gla.fCross.v3f32.v3f32.v3f32(<3 x float> %246, <3 x float> %253)
  %255 = extractelement <3 x float> %254, i32 0
  %256 = load float* %f
  %f62 = fadd float %256, %255
  store float %f62, float* %f
  %257 = load float* %f
  %258 = load float addrspace(2)* @uf, !gla.uniform !4
  %259 = fcmp oeq float %257, %258
  %260 = load float* %f
  %261 = load float addrspace(2)* @uf, !gla.uniform !4
  %262 = fcmp one float %260, %261
  %263 = load float* %f
  %264 = fcmp one float %263, 2.000000e+00
  %265 = and i1 %262, %264
  %266 = or i1 %259, %265
  br i1 %266, label %then63, label %ifmerge65

then63:                                           ; preds = %ifmerge
  %267 = load float* %f
  %f64 = fadd float %267, 1.000000e+00
  store float %f64, float* %f
  br label %ifmerge65

ifmerge65:                                        ; preds = %ifmerge, %then63
  %268 = load i32 addrspace(2)* @ui, !gla.uniform !3
  %269 = load i32* %i
  %i66 = and i32 %269, %268
  store i32 %i66, i32* %i
  %270 = load i32* %i
  %i67 = or i32 %270, 66
  store i32 %i67, i32* %i
  %271 = load i32 addrspace(2)* @ui, !gla.uniform !3
  %272 = load i32* %i
  %i68 = xor i32 %272, %271
  store i32 %i68, i32* %i
  %273 = load i32* %i
  %i69 = srem i32 %273, 17
  store i32 %i69, i32* %i
  %274 = load i32* %i
  %i70 = ashr i32 %274, 2
  store i32 %i70, i32* %i
  %275 = load i32 addrspace(2)* @ui, !gla.uniform !3
  %276 = load i32* %i
  %i71 = shl i32 %276, %275
  store i32 %i71, i32* %i
  %277 = load i32* %i
  %i72 = xor i32 %277, -1
  store i32 %i72, i32* %i
  %278 = load i1* %b
  %b73 = xor i1 %278, true
  store i1 %b73, i1* %b
  %279 = load i1* %b
  br i1 %279, label %then74, label %else

then74:                                           ; preds = %ifmerge65
  %280 = load i32* %i
  %281 = sitofp i32 %280 to float
  %282 = load <4 x float>* %constructed
  %283 = insertelement <4 x float> undef, float %281, i32 0
  %284 = insertelement <4 x float> %283, float %281, i32 1
  %285 = insertelement <4 x float> %284, float %281, i32 2
  %286 = insertelement <4 x float> %285, float %281, i32 3
  %287 = load float* %f
  %288 = load <4 x float>* %constructed75
  %289 = insertelement <4 x float> undef, float %287, i32 0
  %290 = insertelement <4 x float> %289, float %287, i32 1
  %291 = insertelement <4 x float> %290, float %287, i32 2
  %292 = insertelement <4 x float> %291, float %287, i32 3
  %293 = fadd <4 x float> %286, %292
  %294 = load <4 x float>* %v
  %ternary76 = fadd <4 x float> %293, %294
  store <4 x float> %ternary76, <4 x float>* %ternary
  br label %ifmerge78

else:                                             ; preds = %ifmerge65
  %ternary77 = load <4 x float>* %v
  store <4 x float> %ternary77, <4 x float>* %ternary
  br label %ifmerge78

ifmerge78:                                        ; preds = %else, %then74
  %gl_FragColor = load <4 x float>* %ternary
  store <4 x float> %gl_FragColor, <4 x float>* @gl_FragColor
  br label %stage-epilogue

stage-epilogue:                                   ; preds = %ifmerge78
  br label %stage-exit

stage-exit:                                       ; preds = %stage-epilogue
  ret void
}

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fRadians.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fDegrees.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSin.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fCos.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fTan.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAsin.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAcos.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAtan.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSinh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fCosh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fTanh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAsinh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAcosh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAtanh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fPow.v4f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fExp.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fLog.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fExp2.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fLog2.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSqrt.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fInverseSqrt.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAbs.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSign.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fFloor.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fCeiling.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fFraction.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fMin.v4f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fMax.v4f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fClamp.v4f32.v4f32.v4f32.v4f32(<4 x float>, <4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fMix.v4f32.v4f32.v4f32.v4f32(<4 x float>, <4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fStep.v4f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSmoothStep.v4f32.v4f32.v4f32.v4f32(<4 x float>, <4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fStep.v4f32.f32.v4f32(float, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSmoothStep.v4f32.f32.f32.v4f32(float, float, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fNormalize.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fFaceForward.v4f32.v4f32.v4f32.v4f32(<4 x float>, <4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fReflect.v4f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fRefract.v4f32.v4f32.v4f32.f32(<4 x float>, <4 x float>, float) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fDFdx.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fDFdy.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fFilterWidth.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare i1 @llvm.gla.any.v4i1(<4 x i1>) #0

; Function Attrs: nounwind readnone
declare i1 @llvm.gla.all.v4i1(<4 x i1>) #0

; Function Attrs: nounwind readnone
declare float @llvm.gla.fLength.f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare float @llvm.gla.fDistance.f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare float @llvm.gla.fDot4.f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <3 x float> @llvm.gla.fCross.v3f32.v3f32.v3f32(<3 x float>, <3 x float>) #0

attributes #0 = { nounwind readnone }

!gla.entrypoint = !{!0}
!gla.uniforms = !{!1, !3, !4, !5, !6, !7, !8}
!gla.outputs = !{!9}
!gla.noStaticUse = !{!7, !8}

!0 = metadata !{metadata !"main", i32 15}
!1 = metadata !{metadata !"uv4", i32 12, <4 x float>* @uv4_typeProxy, metadata !2}
!2 = metadata !{i32 0, i32 0, i32 1024, null}
!3 = metadata !{metadata !"ui", i32 12, i32* @ui_typeProxy, metadata !2}
!4 = metadata !{metadata !"uf", i32 12, float* @uf_typeProxy, metadata !2}
!5 = metadata !{metadata !"ub41", i32 12, <4 x i1>* @ub41_typeProxy, metadata !2}
!6 = metadata !{metadata !"ub42", i32 12, <4 x i1>* @ub42_typeProxy, metadata !2}
!7 = metadata !{metadata !"uiv4", i32 12, <4 x i32>* @uiv4_typeProxy, metadata !2}
!8 = metadata !{metadata !"ub", i32 12, i1* @ub_typeProxy, metadata !2}
!9 = metadata !{metadata !"gl_FragColor", i32 7, <4 x float>* @gl_FragColor_typeProxy, metadata !10}
!10 = metadata !{i32 0, i32 0, i32 1024, null, i32 0}


Bottom IR:
; ModuleID = 'Glslang'

@uv4 = external addrspace(2) constant <4 x float>
@ui = external addrspace(2) constant i32
@uf = external addrspace(2) constant float
@ub41 = external addrspace(2) constant <4 x i1>
@ub42 = external addrspace(2) constant <4 x i1>
@gl_FragColor = global <4 x float> zeroinitializer

define fastcc void @main() {
entry:
  %0 = load <4 x float> addrspace(2)* @uv4, align 16, !gla.uniform !1
  %1 = fmul <4 x float> %0, <float 0x3F91DF46A0000000, float 0x3F91DF46A0000000, float 0x3F91DF46A0000000, float 0x3F91DF46A0000000>
  %2 = fmul <4 x float> %1, <float 0x404CA5DC20000000, float 0x404CA5DC20000000, float 0x404CA5DC20000000, float 0x404CA5DC20000000>
  %v2 = fadd <4 x float> %1, %2
  %3 = load i32 addrspace(2)* @ui, align 4, !gla.uniform !3
  %i3 = mul i32 %3, %3
  %4 = call <4 x float> @llvm.gla.fSin.v4f32.v4f32(<4 x float> %v2)
  %v4 = fadd <4 x float> %4, %v2
  %5 = call <4 x float> @llvm.gla.fCos.v4f32.v4f32(<4 x float> %v4)
  %v5 = fadd <4 x float> %5, %v4
  %6 = call <4 x float> @llvm.gla.fTan.v4f32.v4f32(<4 x float> %v5)
  %v6 = fadd <4 x float> %6, %v5
  %7 = call <4 x float> @llvm.gla.fAsin.v4f32.v4f32(<4 x float> %v6)
  %v7 = fadd <4 x float> %7, %v6
  %8 = call <4 x float> @llvm.gla.fAcos.v4f32.v4f32(<4 x float> %v7)
  %v8 = fadd <4 x float> %8, %v7
  %9 = call <4 x float> @llvm.gla.fAtan.v4f32.v4f32(<4 x float> %v8)
  %v9 = fadd <4 x float> %9, %v8
  %10 = call <4 x float> @llvm.gla.fSinh.v4f32.v4f32(<4 x float> %v9)
  %v10 = fadd <4 x float> %10, %v9
  %11 = call <4 x float> @llvm.gla.fCosh.v4f32.v4f32(<4 x float> %v10)
  %v11 = fadd <4 x float> %11, %v10
  %12 = call <4 x float> @llvm.gla.fTanh.v4f32.v4f32(<4 x float> %v11)
  %v12 = fadd <4 x float> %12, %v11
  %13 = call <4 x float> @llvm.gla.fAsinh.v4f32.v4f32(<4 x float> %v12)
  %v13 = fadd <4 x float> %13, %v12
  %14 = call <4 x float> @llvm.gla.fAcosh.v4f32.v4f32(<4 x float> %v13)
  %v14 = fadd <4 x float> %14, %v13
  %15 = call <4 x float> @llvm.gla.fAtanh.v4f32.v4f32(<4 x float> %v14)
  %v15 = fadd <4 x float> %15, %v14
  %16 = call <4 x float> @llvm.gla.fPow.v4f32.v4f32.v4f32(<4 x float> %v15, <4 x float> %v15)
  %v16 = fadd <4 x float> %16, %v15
  %17 = call <4 x float> @llvm.gla.fExp.v4f32.v4f32(<4 x float> %v16)
  %v17 = fadd <4 x float> %17, %v16
  %18 = call <4 x float> @llvm.gla.fLog.v4f32.v4f32(<4 x float> %v17)
  %v18 = fadd <4 x float> %18, %v17
  %19 = call <4 x float> @llvm.gla.fExp2.v4f32.v4f32(<4 x float> %v18)
  %v19 = fadd <4 x float> %19, %v18
  %20 = call <4 x float> @llvm.gla.fLog2.v4f32.v4f32(<4 x float> %v19)
  %v20 = fadd <4 x float> %20, %v19
  %21 = call <4 x float> @llvm.gla.fSqrt.v4f32.v4f32(<4 x float> %v20)
  %v21 = fadd <4 x float> %21, %v20
  %22 = call <4 x float> @llvm.gla.fInverseSqrt.v4f32.v4f32(<4 x float> %v21)
  %v22 = fadd <4 x float> %22, %v21
  %23 = call <4 x float> @llvm.gla.fAbs.v4f32.v4f32(<4 x float> %v22)
  %v23 = fadd <4 x float> %23, %v22
  %24 = call <4 x float> @llvm.gla.fSign.v4f32.v4f32(<4 x float> %v23)
  %v24 = fadd <4 x float> %24, %v23
  %25 = call <4 x float> @llvm.gla.fFloor.v4f32.v4f32(<4 x float> %v24)
  %v25 = fadd <4 x float> %25, %v24
  %26 = call <4 x float> @llvm.gla.fCeiling.v4f32.v4f32(<4 x float> %v25)
  %v26 = fadd <4 x float> %26, %v25
  %27 = call <4 x float> @llvm.gla.fFraction.v4f32.v4f32(<4 x float> %v26)
  %v27 = fadd <4 x float> %27, %v26
  %28 = frem <4 x float> %v27, %v27
  %v28 = fadd <4 x float> %28, %v27
  %29 = call <4 x float> @llvm.gla.fSwizzle.v4f32.v4f32.v4i32(<4 x float> %v28, <4 x i32> zeroinitializer)
  %30 = frem <4 x float> %v28, %29
  %v29 = fadd <4 x float> %30, %v28
  %31 = call <4 x float> @llvm.gla.fMin.v4f32.v4f32.v4f32(<4 x float> %v29, <4 x float> %0)
  %v30 = fadd <4 x float> %31, %v29
  %32 = call <4 x float> @llvm.gla.fMax.v4f32.v4f32.v4f32(<4 x float> %v30, <4 x float> %0)
  %v31 = fadd <4 x float> %32, %v30
  %33 = call <4 x float> @llvm.gla.fClamp.v4f32.v4f32.v4f32.v4f32(<4 x float> %v31, <4 x float> %0, <4 x float> %0)
  %v32 = fadd <4 x float> %33, %v31
  %34 = call <4 x float> @llvm.gla.fMix.v4f32.v4f32.v4f32.v4f32(<4 x float> %v32, <4 x float> %v32, <4 x float> %v32)
  %v33 = fadd <4 x float> %34, %v32
  %35 = call <4 x float> @llvm.gla.fStep.v4f32.v4f32.v4f32(<4 x float> %v33, <4 x float> %v33)
  %v34 = fadd <4 x float> %35, %v33
  %36 = call <4 x float> @llvm.gla.fSmoothStep.v4f32.v4f32.v4f32.v4f32(<4 x float> %v34, <4 x float> %v34, <4 x float> %v34)
  %v35 = fadd <4 x float> %36, %v34
  %37 = load float addrspace(2)* @uf, align 4, !gla.uniform !4
  %38 = call <4 x float> @llvm.gla.fStep.v4f32.f32.v4f32(float %37, <4 x float> %v35)
  %v36 = fadd <4 x float> %v35, %38
  %39 = call <4 x float> @llvm.gla.fSmoothStep.v4f32.f32.f32.v4f32(float %37, float %37, <4 x float> %v36)
  %v37 = fadd <4 x float> %v36, %39
  %40 = call <4 x float> @llvm.gla.fNormalize.v4f32.v4f32(<4 x float> %v37)
  %v38 = fadd <4 x float> %v37, %40
  %41 = call <4 x float> @llvm.gla.fFaceForward.v4f32.v4f32.v4f32.v4f32(<4 x float> %v38, <4 x float> %v38, <4 x float> %v38)
  %v39 = fadd <4 x float> %v38, %41
  %42 = call <4 x float> @llvm.gla.fReflect.v4f32.v4f32.v4f32(<4 x float> %v39, <4 x float> %v39)
  %v40 = fadd <4 x float> %v39, %42
  %43 = call <4 x float> @llvm.gla.fRefract.v4f32.v4f32.v4f32.f32(<4 x float> %v40, <4 x float> %v40, float %37)
  %v41 = fadd <4 x float> %v40, %43
  %44 = call <4 x float> @llvm.gla.fDFdx.v4f32.v4f32(<4 x float> %v41)
  %v42 = fadd <4 x float> %v41, %44
  %45 = call <4 x float> @llvm.gla.fDFdy.v4f32.v4f32(<4 x float> %v42)
  %v43 = fadd <4 x float> %v42, %45
  %46 = call <4 x float> @llvm.gla.fFilterWidth.v4f32.v4f32(<4 x float> %v43)
  %v44 = fadd <4 x float> %v43, %46
  %47 = call <3 x float> @llvm.gla.fSwizzle.v3f32.v4f32.v3i32(<4 x float> %v44, <3 x i32> <i32 0, i32 1, i32 2>)
  %48 = fcmp olt <4 x float> %v44, %0
  %b45 = call i1 @llvm.gla.any.v4i1(<4 x i1> %48)
  %49 = fcmp ole <4 x float> %v44, %0
  %50 = call i1 @llvm.gla.any.v4i1(<4 x i1> %49)
  %b46 = and i1 %b45, %50
  %51 = fcmp ogt <4 x float> %v44, %0
  %52 = call i1 @llvm.gla.any.v4i1(<4 x i1> %51)
  %b47 = and i1 %b46, %52
  %53 = fcmp oge <4 x float> %v44, %0
  %54 = call i1 @llvm.gla.any.v4i1(<4 x i1> %53)
  %b48 = and i1 %b47, %54
  %55 = load <4 x i1> addrspace(2)* @ub41, align 4, !gla.uniform !5
  %56 = load <4 x i1> addrspace(2)* @ub42, align 4, !gla.uniform !6
  %57 = icmp eq <4 x i1> %55, %56
  %58 = call i1 @llvm.gla.any.v4i1(<4 x i1> %57)
  %b49 = and i1 %b48, %58
  %59 = icmp ne <4 x i1> %55, %56
  %60 = call i1 @llvm.gla.any.v4i1(<4 x i1> %59)
  %b50 = and i1 %b49, %60
  %61 = call i1 @llvm.gla.any.v4i1(<4 x i1> %55)
  %b51 = and i1 %b50, %61
  %62 = call i1 @llvm.gla.all.v4i1(<4 x i1> %55)
  %b52 = and i1 %b51, %62
  %63 = xor <4 x i1> %55, <i1 true, i1 true, i1 true, i1 true>
  %64 = call i1 @llvm.gla.any.v4i1(<4 x i1> %63)
  %b53 = and i1 %b52, %64
  %65 = add i32 %i3, %3
  %66 = mul i32 %65, %i3
  %67 = sub i32 %66, %3
  %i54 = sdiv i32 %67, %i3
  %i55 = srem i32 %i54, %3
  %68 = icmp eq i32 %i55, 2
  %i56 = add i32 %i55, 1
  %.i56 = select i1 %68, i32 0, i32 %i56
  %69 = fadd float %37, %37
  %70 = fmul float %37, %69
  %71 = fsub float %70, %37
  %f57 = fdiv float %71, %37
  %72 = call float @llvm.gla.fLength.f32.v4f32(<4 x float> %v44)
  %f58 = fadd float %f57, %72
  %73 = call float @llvm.gla.fDot4.f32.v4f32.v4f32(<4 x float> %v44, <4 x float> %v44)
  %f60 = fadd float %f58, %73
  %74 = fmul float %37, %f60
  %f61 = fadd float %f60, %74
  %75 = call <3 x float> @llvm.gla.fCross.v3f32.v3f32.v3f32(<3 x float> %47, <3 x float> %47)
  %76 = extractelement <3 x float> %75, i32 0
  %f62 = fadd float %76, %f61
  %77 = fcmp oeq float %f62, %37
  %78 = fcmp one float %f62, %37
  %79 = fcmp one float %f62, 2.000000e+00
  %80 = and i1 %78, %79
  %81 = or i1 %77, %80
  %f64 = fadd float %f62, 1.000000e+00
  %select79 = select i1 %81, float %f64, float %f62
  %82 = call <4 x float> @llvm.gla.fSwizzle.v4f32.f32.v4i32(float %select79, <4 x i32> zeroinitializer)
  %83 = load i32 addrspace(2)* @ui, align 4, !gla.uniform !3
  %i66 = and i32 %83, %.i56
  %i67 = or i32 %i66, 66
  %i68 = xor i32 %i67, %83
  %i69 = srem i32 %i68, 17
  %i70 = ashr i32 %i69, 2
  %i71 = shl i32 %i70, %83
  %i72 = xor i32 %i71, -1
  %84 = sitofp i32 %i72 to float
  %85 = call <4 x float> @llvm.gla.fSwizzle.v4f32.f32.v4i32(float %84, <4 x i32> zeroinitializer)
  %86 = fadd <4 x float> %85, %82
  %ternary76 = fadd <4 x float> %v44, %86
  %select = select i1 %b53, <4 x float> %v44, <4 x float> %ternary76
  store <4 x float> %select, <4 x float>* @gl_FragColor, align 16
  br label %stage-epilogue

stage-epilogue:                                   ; preds = %entry
  br label %stage-exit

stage-exit:                                       ; preds = %stage-epilogue
  ret void
}

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSin.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fCos.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fTan.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAsin.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAcos.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAtan.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSinh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fCosh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fTanh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAsinh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAcosh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAtanh.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fPow.v4f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fExp.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fLog.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fExp2.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fLog2.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSqrt.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fInverseSqrt.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fAbs.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSign.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fFloor.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fCeiling.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fFraction.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fMin.v4f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fMax.v4f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fClamp.v4f32.v4f32.v4f32.v4f32(<4 x float>, <4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fMix.v4f32.v4f32.v4f32.v4f32(<4 x float>, <4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fStep.v4f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSmoothStep.v4f32.v4f32.v4f32.v4f32(<4 x float>, <4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fStep.v4f32.f32.v4f32(float, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSmoothStep.v4f32.f32.f32.v4f32(float, float, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fNormalize.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fFaceForward.v4f32.v4f32.v4f32.v4f32(<4 x float>, <4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fReflect.v4f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fRefract.v4f32.v4f32.v4f32.f32(<4 x float>, <4 x float>, float) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fDFdx.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fDFdy.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fFilterWidth.v4f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare i1 @llvm.gla.any.v4i1(<4 x i1>) #0

; Function Attrs: nounwind readnone
declare i1 @llvm.gla.all.v4i1(<4 x i1>) #0

; Function Attrs: nounwind readnone
declare float @llvm.gla.fLength.f32.v4f32(<4 x float>) #0

; Function Attrs: nounwind readnone
declare float @llvm.gla.fDot4.f32.v4f32.v4f32(<4 x float>, <4 x float>) #0

; Function Attrs: nounwind readnone
declare <3 x float> @llvm.gla.fCross.v3f32.v3f32.v3f32(<3 x float>, <3 x float>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSwizzle.v4f32.v4f32.v4i32(<4 x float>, <4 x i32>) #0

; Function Attrs: nounwind readnone
declare <3 x float> @llvm.gla.fSwizzle.v3f32.v4f32.v3i32(<4 x float>, <3 x i32>) #0

; Function Attrs: nounwind readnone
declare <4 x float> @llvm.gla.fSwizzle.v4f32.f32.v4i32(float, <4 x i32>) #0

attributes #0 = { nounwind readnone }

!gla.entrypoint = !{!0}
!gla.uniforms = !{!1, !3, !4, !5, !6, !7, !8}
!gla.outputs = !{!9}
!gla.noStaticUse = !{!7, !8}

!0 = metadata !{metadata !"main", i32 15}
!1 = metadata !{metadata !"uv4", i32 12, <4 x float>* @uv4_typeProxy, metadata !2}
!2 = metadata !{i32 0, i32 0, i32 1024, null}
!3 = metadata !{metadata !"ui", i32 12, i32* @ui_typeProxy, metadata !2}
!4 = metadata !{metadata !"uf", i32 12, float* @uf_typeProxy, metadata !2}
!5 = metadata !{metadata !"ub41", i32 12, <4 x i1>* @ub41_typeProxy, metadata !2}
!6 = metadata !{metadata !"ub42", i32 12, <4 x i1>* @ub42_typeProxy, metadata !2}
!7 = metadata !{metadata !"uiv4", i32 12, <4 x i32>* @uiv4_typeProxy, metadata !2}
!8 = metadata !{metadata !"ub", i32 12, i1* @ub_typeProxy, metadata !2}
!9 = metadata !{metadata !"gl_FragColor", i32 7, <4 x float>* @gl_FragColor_typeProxy, metadata !10}
!10 = metadata !{i32 0, i32 0, i32 1024, null, i32 0}
#version 130
// LunarGOO output
uniform vec4 uv4;
uniform int ui;
uniform float uf;
uniform bvec4 ub41;
uniform bvec4 ub42;
uniform ivec4 uiv4;
uniform bool ub;
const vec4 C_vssely1 = vec4(0.0174533);
const vec4 C_4bmlfm = vec4(57.2958);
const int C_2 = 2;
const int C_1 = 1;
const int C_0 = 0;
const float C_2d0 = 2.0;
const float C_1d0 = 1.0;
const int C_66 = 66;
const int C_17 = 17;

void main()
{
	vec4 H_hgy73b1 = uv4 * C_vssely1;
	vec4 H_9zy93p = (H_hgy73b1 * C_4bmlfm) + H_hgy73b1;
	int H_h8ktht1 = ui * ui;
	vec4 H_znp065 = sin(H_9zy93p);
	vec4 H_x4gq9l = H_9zy93p + H_znp065;
	vec4 H_nz828d = cos(H_x4gq9l);
	vec4 H_5ij1lm1 = H_nz828d + H_x4gq9l;
	vec4 H_j8sqro1 = tan(H_5ij1lm1);
	vec4 H_timra01 = H_5ij1lm1 + H_j8sqro1;
	vec4 H_4v8id81 = asin(H_timra01);
	vec4 H_mj0a81 = H_4v8id81 + H_timra01;
	vec4 H_2ct50b = acos(H_mj0a81);
	vec4 H_vi9qzx1 = H_2ct50b + H_mj0a81;
	vec4 H_34myrv1 = atan(H_vi9qzx1);
	vec4 H_ingn9g = H_34myrv1 + H_vi9qzx1;
	vec4 H_poivza = sinh(H_ingn9g);
	vec4 H_rf5mkp1 = H_ingn9g + H_poivza;
	vec4 H_m17vev1 = cosh(H_rf5mkp1);
	vec4 H_xjy9ol = H_m17vev1 + H_rf5mkp1;
	vec4 H_pua0qb1 = tanh(H_xjy9ol);
	vec4 H_dy915y = H_pua0qb1 + H_xjy9ol;
	vec4 H_ff7ziz = asinh(H_dy915y);
	vec4 H_56gagc1 = H_dy915y + H_ff7ziz;
	vec4 H_lviw0r = acosh(H_56gagc1);
	vec4 H_qvy234 = H_56gagc1 + H_lviw0r;
	vec4 H_s7lpo1 = atanh(H_qvy234);
	vec4 H_9fwr1h = H_qvy234 + H_s7lpo1;
	vec4 H_202hrr = pow(H_9fwr1h, H_9fwr1h);
	vec4 H_pillmb = H_202hrr + H_9fwr1h;
	vec4 H_03ad9k1 = exp(H_pillmb);
	vec4 H_bayowv1 = H_03ad9k1 + H_pillmb;
	vec4 H_4ifk4m = log(H_bayowv1);
	vec4 H_28bw9s1 = H_4ifk4m + H_bayowv1;
	vec4 H_81wrvh = exp2(H_28bw9s1);
	vec4 H_2vzjg2 = H_28bw9s1 + H_81wrvh;
	vec4 H_6o2tkl = log2(H_2vzjg2);
	vec4 H_b678ll1 = H_2vzjg2 + H_6o2tkl;
	vec4 H_ps0scn = sqrt(H_b678ll1);
	vec4 H_x7j2rg1 = H_b678ll1 + H_ps0scn;
	vec4 H_yjke7p1 = inversesqrt(H_x7j2rg1);
	vec4 H_4f2dob = H_x7j2rg1 + H_yjke7p1;
	vec4 H_c7gqu91 = abs(H_4f2dob);
	vec4 H_o3xn7x1 = H_4f2dob + H_c7gqu91;
	vec4 H_we3ku51 = sign(H_o3xn7x1);
	vec4 H_nk9zph1 = H_o3xn7x1 + H_we3ku51;
	vec4 H_cgdgod1 = floor(H_nk9zph1);
	vec4 H_yt5ew81 = H_cgdgod1 + H_nk9zph1;
	vec4 H_biw3r5 = ceil(H_yt5ew81);
	vec4 H_vofgcr = H_biw3r5 + H_yt5ew81;
	vec4 H_46133f = fract(H_vofgcr);
	vec4 H_m6s1y = H_46133f + H_vofgcr;
	vec4 H_m04o571 = mod(H_m6s1y, H_m6s1y);
	vec4 H_3cz9gy1 = H_m04o571 + H_m6s1y;
	vec4 H_4f3gp11 = H_3cz9gy1.xxxx;
	vec4 H_mhsys71 = mod(H_3cz9gy1, H_4f3gp11);
	vec4 H_2qsccr1 = H_3cz9gy1 + H_mhsys71;
	vec4 H_1xr9pe = min(H_2qsccr1, uv4);
	vec4 H_g2ool91 = H_1xr9pe + H_2qsccr1;
	vec4 H_5ipioj = max(H_g2ool91, uv4);
	vec4 H_10cze21 = H_5ipioj + H_g2ool91;
	vec4 H_mmakfj1 = clamp(H_10cze21, uv4, uv4);
	vec4 H_tq9aho = H_10cze21 + H_mmakfj1;
	vec4 H_5bnvqy1 = mix(H_tq9aho, H_tq9aho, H_tq9aho);
	vec4 H_yqurkm1 = H_5bnvqy1 + H_tq9aho;
	vec4 H_yavppg1 = step(H_yqurkm1, H_yqurkm1);
	vec4 H_as8vet = H_yavppg1 + H_yqurkm1;
	vec4 H_kvy92o = smoothstep(H_as8vet, H_as8vet, H_as8vet);
	vec4 H_frspzk = H_as8vet + H_kvy92o;
	vec4 H_e16aod1 = step(uf, H_frspzk);
	vec4 H_br91ih1 = H_e16aod1 + H_frspzk;
	vec4 H_nwjh081 = smoothstep(uf, uf, H_br91ih1);
	vec4 H_0w97xo1 = H_br91ih1 + H_nwjh081;
	vec4 H_7u05m31 = normalize(H_0w97xo1);
	vec4 H_nqpvc = H_0w97xo1 + H_7u05m31;
	vec4 H_jbgvxv = faceforward(H_nqpvc, H_nqpvc, H_nqpvc);
	vec4 H_xigwny = H_jbgvxv + H_nqpvc;
	vec4 H_dvc7ak1 = reflect(H_xigwny, H_xigwny);
	vec4 H_fv7t4x1 = H_dvc7ak1 + H_xigwny;
	vec4 H_glav1m1 = refract(H_fv7t4x1, H_fv7t4x1, uf);
	vec4 H_hv5khw1 = H_fv7t4x1 + H_glav1m1;
	vec4 H_5j41nb1 = dFdx(H_hv5khw1);
	vec4 H_rhc1gw = H_5j41nb1 + H_hv5khw1;
	vec4 H_0hq7v4 = dFdy(H_rhc1gw);
	vec4 H_crilq01 = H_0hq7v4 + H_rhc1gw;
	vec4 H_rqx4ra1 = fwidth(H_crilq01);
	vec4 H_1lkc4l = H_crilq01 + H_rqx4ra1;
	vec3 H_3qg4md = vec3(H_1lkc4l);
	bool H_hn09bq1 = any((lessThan(H_1lkc4l, uv4)));
	bool H_7yltct = any((lessThanEqual(H_1lkc4l, uv4)));
	bool H_mptf0b1 = any((greaterThan(H_1lkc4l, uv4)));
	bool H_uao5pj = any((greaterThanEqual(H_1lkc4l, uv4)));
	bool H_lkxh281 = any((equal(ub41, ub42)));
	bool H_c8o8qv1 = any((notEqual(ub41, ub42)));
	bool H_u0wve91 = any(ub41);
	bool H_f4b5uh1 = all(ub41);
	bool H_bkp4gy1 = any((not(ub41)));
	bool H_lkbmnm1 = (((((((H_hn09bq1 && H_7yltct) && H_mptf0b1) && H_uao5pj) && H_lkxh281) && H_c8o8qv1) && H_u0wve91) && H_f4b5uh1) && H_bkp4gy1;
	int H_gxb34c1 = ((((H_h8ktht1 + ui) * H_h8ktht1) - ui) / H_h8ktht1) % ui;
	float H_wiio49 = length(H_1lkc4l);
	float H_nis2gp = dot(H_1lkc4l, H_1lkc4l);
	float H_a8jqwu = (((((uf + uf) * uf) - uf) / uf) + H_wiio49) + H_nis2gp;
	vec3 H_k3rgmp1 = cross(H_3qg4md, H_3qg4md);
	float H_u2zfs2 = ((H_a8jqwu * uf) + H_a8jqwu) + H_k3rgmp1.x;
	vec4 H_3c73yp = vec4((((H_u2zfs2 == uf) || ((H_u2zfs2 != uf) && (H_u2zfs2 != C_2d0))) ? (H_u2zfs2 + C_1d0) : H_u2zfs2));
	vec4 H_hf575k1 = vec4((float((~(((((((ui & ((H_gxb34c1 == C_2) ? C_0 : (H_gxb34c1 + C_1))) | C_66) ^ ui) % C_17) >> C_2) << ui))))));
	gl_FragColor = (H_lkbmnm1 ? H_1lkc4l : ((H_3c73yp + H_hf575k1) + H_1lkc4l));
	
}

#version 130
// LunarGOO output
uniform vec4 uv4;
uniform int ui;
uniform float uf;
uniform bvec4 ub41;
uniform bvec4 ub42;
uniform ivec4 uiv4;
uniform bool ub;
const vec4 C_vssely1 = vec4(0.0174533);
const vec4 C_4bmlfm = vec4(57.2958);
const float C_2d0 = 2.0;
const float C_1d0 = 1.0;
const int C_2 = 2;
const int C_1 = 1;
const int C_0 = 0;
const int C_66 = 66;
const int C_17 = 17;

void main()
{
	vec4 H_hgy_c1 = uv4 * C_vssely1;
	vec4 H_uc2 = (H_hgy_c1 * C_4bmlfm) + H_hgy_c1;
	int H_h_c3 = ui * ui;
	vec4 H_znp_c4 = sin(H_uc2);
	vec4 H_x_c5 = H_uc2 + H_znp_c4;
	vec4 H_nz_c6 = cos(H_x_c5);
	vec4 H_uc7 = H_nz_c6 + H_x_c5;
	vec4 H_j_c8 = tan(H_uc7);
	vec4 H_timra_c9 = H_j_c8 + H_uc7;
	vec4 H_uc10 = asin(H_timra_c9);
	vec4 H_mj_c11 = H_timra_c9 + H_uc10;
	vec4 H_uc12 = acos(H_mj_c11);
	vec4 H_vi_c13 = H_mj_c11 + H_uc12;
	vec4 H_uc14 = atan(H_vi_c13);
	vec4 H_ingn_c15 = H_uc14 + H_vi_c13;
	vec4 H_poivza_c16 = sinh(H_ingn_c15);
	vec4 H_rf_c17 = H_ingn_c15 + H_poivza_c16;
	vec4 H_m_c18 = cosh(H_rf_c17);
	vec4 H_xjy_c19 = H_m_c18 + H_rf_c17;
	vec4 H_pua_c20 = tanh(H_xjy_c19);
	vec4 H_dy_c21 = H_pua_c20 + H_xjy_c19;
	vec4 H_ff_c22 = asinh(H_dy_c21);
	vec4 H_uc23 = H_dy_c21 + H_ff_c22;
	vec4 H_lviw_c24 = acosh(H_uc23);
	vec4 H_qvy_c25 = H_lviw_c24 + H_uc23;
	vec4 H_s_c26 = atanh(H_qvy_c25);
	vec4 H_uc27 = H_qvy_c25 + H_s_c26;
	vec4 H_uc28 = pow(H_uc27, H_uc27);
	vec4 H_pillmb_c29 = H_uc27 + H_uc28;
	vec4 H_uc30 = exp(H_pillmb_c29);
	vec4 H_bayowv_c31 = H_pillmb_c29 + H_uc30;
	vec4 H_uc32 = log(H_bayowv_c31);
	vec4 H_uc33 = H_bayowv_c31 + H_uc32;
	vec4 H_uc34 = exp2(H_uc33);
	vec4 H_uc35 = H_uc33 + H_uc34;
	vec4 H_uc36 = log2(H_uc35);
	vec4 H_b_c37 = H_uc35 + H_uc36;
	vec4 H_ps_c38 = sqrt(H_b_c37);
	vec4 H_x_c39 = H_b_c37 + H_ps_c38;
	vec4 H_yjke_c40 = inversesqrt(H_x_c39);
	vec4 H_uc41 = H_x_c39 + H_yjke_c40;
	vec4 H_c42 = abs(H_uc41);
	vec4 H_o_c43 = H_c42 + H_uc41;
	vec4 H_we_c44 = sign(H_o_c43);
	vec4 H_nk_c45 = H_o_c43 + H_we_c44;
	vec4 H_cgdgod_c46 = floor(H_nk_c45);
	vec4 H_yt_c47 = H_cgdgod_c46 + H_nk_c45;
	vec4 H_biw_c48 = ceil(H_yt_c47);
	vec4 H_vofgcr_c49 = H_biw_c48 + H_yt_c47;
	vec4 H_uc50 = fract(H_vofgcr_c49);
	vec4 H_m_c51 = H_uc50 + H_vofgcr_c49;
	vec4 H_m_c52 = mod(H_m_c51, H_m_c51);
	vec4 H_uc53 = H_m_c51 + H_m_c52;
	vec4 H_upzqxw1 = H_uc53.xxxx;
	vec4 H_mhsys_c54 = mod(H_uc53, H_upzqxw1);
	vec4 H_uc55 = H_mhsys_c54 + H_uc53;
	vec4 H_uc56 = min(H_uc55, uv4);
	vec4 H_g_c57 = H_uc55 + H_uc56;
	vec4 H_uc58 = max(H_g_c57, uv4);
	vec4 H_uc59 = H_g_c57 + H_uc58;
	vec4 H_mmakfj_c60 = clamp(H_uc59, uv4, uv4);
	vec4 H_tq_c61 = H_mmakfj_c60 + H_uc59;
	vec4 H_uc62 = mix(H_tq_c61, H_tq_c61, H_tq_c61);
	vec4 H_yqurkm_c63 = H_tq_c61 + H_uc62;
	vec4 H_yavppg_c64 = step(H_yqurkm_c63, H_yqurkm_c63);
	vec4 H_as_c65 = H_yavppg_c64 + H_yqurkm_c63;
	vec4 H_kvy_c66 = smoothstep(H_as_c65, H_as_c65, H_as_c65);
	vec4 H_frspzk_c67 = H_as_c65 + H_kvy_c66;
	vec4 H_e_c68 = step(uf, H_frspzk_c67);
	vec4 H_br_c69 = H_e_c68 + H_frspzk_c67;
	vec4 H_nwjh_c70 = smoothstep(uf, uf, H_br_c69);
	vec4 H_uc71 = H_br_c69 + H_nwjh_c70;
	vec4 H_uc72 = normalize(H_uc71);
	vec4 H_nqpvc_c73 = H_uc71 + H_uc72;
	vec4 H_jbgvxv_c74 = faceforward(H_nqpvc_c73, H_nqpvc_c73, H_nqpvc_c73);
	vec4 H_xigwny_c75 = H_jbgvxv_c74 + H_nqpvc_c73;
	vec4 H_dvc_c76 = reflect(H_xigwny_c75, H_xigwny_c75);
	vec4 H_fv_c77 = H_dvc_c76 + H_xigwny_c75;
	vec4 H_glav_c78 = refract(H_fv_c77, H_fv_c77, uf);
	vec4 H_hv_c79 = H_fv_c77 + H_glav_c78;
	vec4 H_uc80 = dFdx(H_hv_c79);
	vec4 H_rhc_c81 = H_hv_c79 + H_uc80;
	vec4 H_uc82 = dFdy(H_rhc_c81);
	vec4 H_crilq_c83 = H_rhc_c81 + H_uc82;
	vec4 H_rqx_c84 = fwidth(H_crilq_c83);
	vec4 H_uc85 = H_crilq_c83 + H_rqx_c84;
	vec3 H_fnk19f1 = vec3(H_uc85);
	bool H_hn_c86 = any((lessThan(H_uc85, uv4)));
	bool H_uc87 = any((lessThanEqual(H_uc85, uv4)));
	bool H_mptf_c88 = any((greaterThan(H_uc85, uv4)));
	bool H_uao_c89 = any((greaterThanEqual(H_uc85, uv4)));
	bool H_lkxh_c90 = any((equal(ub41, ub42)));
	bool H_c91 = any((notEqual(ub41, ub42)));
	bool H_u_c92 = any(ub41);
	bool H_f_c93 = all(ub41);
	bool H_bkp_c94 = any((not(ub41)));
	int H_gxb_c95 = ((((H_h_c3 + ui) * H_h_c3) - ui) / H_h_c3) % ui;
	float H_wiio_c96 = length(H_uc85);
	float H_nis_c97 = dot(H_uc85, H_uc85);
	float H_a_c98 = (((((uf + uf) * uf) - uf) / uf) + H_wiio_c96) + H_nis_c97;
	vec3 H_k_c99 = cross(H_fnk19f1, H_fnk19f1);
	float H_u_c100 = ((H_a_c98 * uf) + H_a_c98) + H_k_c99.x;
	vec4 H_luwsmv1 = vec4((((H_u_c100 == uf) || ((H_u_c100 != uf) && (H_u_c100 != C_2d0))) ? (H_u_c100 + C_1d0) : H_u_c100));
	vec4 H_3s54sv = vec4((float((~(((((((ui & ((H_gxb_c95 == C_2) ? C_0 : (H_gxb_c95 + C_1))) | C_66) ^ ui) % C_17) >> C_2) << ui))))));
	vec4 select_c101 = ((((((((H_hn_c86 && H_uc87) && H_mptf_c88) && H_uao_c89) && H_lkxh_c90) && H_c91) && H_u_c92) && H_f_c93) && H_bkp_c94) ? H_uc85 : ((H_3s54sv + H_luwsmv1) + H_uc85);
	gl_FragColor = select_c101;
	
}

